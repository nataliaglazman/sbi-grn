# pip install sbi tensor torch

import torch
import sbi
import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt

from sbi.inference import SNLE, prepare_for_sbi, simulate_for_sbi
from sbi import utils as utils
from sbi import analysis as analysis

#####################################################################################################################

# To run for more parameters, we must 1) add the parameter in the params_to_be_inferred list e.g. params_to_be_inferred = ["k1" , "k2", "k3"] 
#                                     2) add the true value of the new parameter in true_params e.g. true_params = 0, 0, 0
#                                     3) add the parameter in theta in the repressilator function e.g. k1, k2, k3 = theta
#                                     4) ensure the repressilator output accounts for the new parameter e.g. -m3 + (10 ** 3 / (1 + (10 ** k3 * p1) ** 2)) + 10 ** 0
#                                     5) ensure consistency in the order of parameters and values e.g. the order of parameters in the
#                                     params_to_be_inferred list must be the same as their order in the definition of theta within the
#                                     repressilator function, and their order must also be refleccted in true_params

#####################################################################################################################

params_to_be_inferred = ["k1" , "k2"]
true_params = 0, 0
prior_min = -3           # same for all parameters
prior_max = 3            # same for all parameters
num_timesteps = 100
num_rounds = 2          # how many rounds of SNLE
num_simulations = 500   # how many simulations in each round

#####################################################################################################################

t = np.linspace(0, 100, num_timesteps)

def my_simulator(theta):
    def repressilator(variables, t, theta):
        m1, p1, m2, p2, m3, p3 = variables
        k1, k2 = theta

        return [-m1 + (10 ** 3 / (1 + (10 ** k1 * p2) ** 2)) + 10 ** 0, #return the results if the six odes
                -10 ** 0 * (p1 - m1),
                -m2 + (10 ** 3 / (1 + (10 ** k2 * p3) ** 2)) + 10 ** 0,
                -10 ** 0 * (p2 - m2),
                -m3 + (10 ** 3 / (1 + (10 ** 0 * p1) ** 2)) + 10 ** 0,
                -10 ** 0 * (p3 - m3)]

    def solve_ode(theta, t):
        initial_conditions = np.array([0, 2, 0, 1, 0, 3], dtype=np.float32)
        solution = odeint(repressilator, initial_conditions, t, args=(theta,))
        return torch.tensor(solution, dtype=torch.float32).flatten()  # Flatten tensor to size [600]
    
    return solve_ode(theta, t)

x_o = my_simulator(true_params) # Generate observations,with data from 100 timepoints for each of 6 variables
                                # concatenated into one tensor (100*6=600)

#####################################################################################################################

num_dim = len(true_params)
prior = utils.BoxUniform(low=prior_min * torch.ones(num_dim), high=prior_max * torch.ones(num_dim))
simulator, prior = prepare_for_sbi(my_simulator, prior)

inference = SNLE(prior)  # Initialise inference
posteriors = []          # Empty list to contain posterior after each round
proposal = prior         # For the first round proposal = prior, then updated (sequentiality)

for _ in range(num_rounds):
    print(f"Round {_+1}")
    theta, x = simulate_for_sbi(simulator, proposal, num_simulations = num_simulations)
    # sample theta from proposal (=posterior of previus round) using MCMC, and use to simulate data x
    density_estimator = inference.append_simulations(theta, x).train()
    # train neural net using (theta, x) sets
    posterior = inference.build_posterior(density_estimator)
    # generate posterior (proportional to prior*neural estimator)
    posteriors.append(posterior)
    proposal = posterior.set_default_x(x_o) # set posterior = proposal for next round

posterior_samples = posterior.sample((500,), x=x_o) # sample to plot the posteriors

# Plot posteriors
_ = analysis.pairplot(posterior_samples, limits=[[prior_min, prior_max]] * num_dim, figsize=(8, 8), labels = params_to_be_inferred)

##################################### PLOT TRAJECTORIES ##############################################################

def plot_trajectories(ax, theta, linestyle='-', color='grey'):
    for param_set in theta:
        traj = my_simulator(param_set).reshape(-1, 6)
        for i in range(6):
            ax.plot(t, traj[:, i], linestyle, color=color, alpha=0.1)

def plot_variables(ax, true_traj, posterior_samples, variables_idx, variables, color_true='blue', color_sim='grey'):
    ax.plot(t, true_traj[:, variables_idx], linestyle='-', color=color_true, label='True trajectory')
    plot_trajectories(ax, theta=posterior_samples, linestyle='--', color=color_sim)
    ax.set_xlabel('t')
    ax.set_ylabel(variables[variables_idx])
    ax.legend()

variables = ["m1", "p1", "m2", "p2", "m3", "p3"]

# Sample final posterior
posterior_samples = posterior.sample((50,), x=x_o)  # Adjust the number of samples as needed (not 500 to avoid confluene)

# Plot trajectories
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for idx, ax in enumerate(axes.flat):
    true_traj = x_o.reshape(-1, 6)  # Reshape flattened observation back to the original shape
    plot_variables(ax, true_traj, posterior_samples, variables_idx=idx, variables=variables)

plt.tight_layout()
plt.show()
