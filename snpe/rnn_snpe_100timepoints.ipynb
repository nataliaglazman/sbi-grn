{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to make the RNN work... with 100 timepoints only..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "\n",
    "def euler_method(func, y0, t, params):\n",
    "    num_eqns = len(y0)\n",
    "    y = np.zeros((len(t), num_eqns))\n",
    "    y[0] = y0\n",
    "    dt = t[1] - t[0]\n",
    "    for i in range(1, len(t)):\n",
    "        y[i] = y[i - 1] + dt * func(t[i - 1], y[i - 1], params)\n",
    "    return y\n",
    "\n",
    "# Example function representing a system of ODEs with parameters\n",
    "def system_of_odes(t, y, params):\n",
    "    dydt = np.zeros_like(y)\n",
    "\n",
    "    m1, p1, m2, p2, m3, p3 = y\n",
    "    k1, k2 = params #only 3 ks are parameters to infer\n",
    "    k3 = 246.96291990024542\n",
    "    a1 = a2 = a3 = 24.78485282457379\n",
    "    g1 = g2 = g3 = 0.024884149937163258\n",
    "    n1 = n2 = n3 = 5\n",
    "    b1 = b2 = b3 = 33.82307682700831\n",
    "    dm1 = dm2 = dm3 = 1.143402097500176\n",
    "    dp1 = dp2 = dp3 = 0.7833664565550977\n",
    "\n",
    "    dydt[0] = -dm1 * m1 + (a1 / (1 + ((1/k1) * p2) ** n1)) + g1\n",
    "    dydt[1] = (b1 * m1) - (dp1 * p1)\n",
    "    dydt[2] = -dm2 * m2 + (a2 / (1 + ((1/k2) * p3) ** n2)) + g2\n",
    "    dydt[3] = (b2 * m2) - (dp2 * p2)\n",
    "    dydt[4] = -dm3 * m3 + (a3 / (1 + ((1/k3) * p1) ** n3)) + g3\n",
    "    dydt[5] = (b3 * m3)-(dp3 * p3)\n",
    "    \n",
    "    return dydt\n",
    "\n",
    "# Initial conditions\n",
    "true_params = [\n",
    "    246.96291990024542, 246.96291990024542]\n",
    "num_timesteps = 100\n",
    "num_trajectories = 6\n",
    "y0 = np.array([0, 1, 0, 3, 0, 2])\n",
    "t = np.linspace(0, 100, num_timesteps)\n",
    "\n",
    "def simulator(parameter_set):\n",
    "    full_tensor = torch.zeros((num_trajectories * num_timesteps)).unsqueeze(0)\n",
    "    if len(parameter_set) == 1:\n",
    "        for params in parameter_set:\n",
    "            y = euler_method(system_of_odes, y0, t, params)\n",
    "            y_transposed = y.T\n",
    "            concatenated_trajectories = y_transposed.flatten()\n",
    "            yt = torch.tensor(concatenated_trajectories)\n",
    "            final_tensor = yt.unsqueeze(0).unsqueeze(-1)\n",
    "            return final_tensor\n",
    "    else:\n",
    "        for params in parameter_set:\n",
    "            y = euler_method(system_of_odes, y0, t, params)\n",
    "            y_transposed = y.T\n",
    "            concatenated_trajectories = y_transposed.flatten()\n",
    "            yt = torch.tensor(concatenated_trajectories)\n",
    "            true_tensor = yt.unsqueeze(0)\n",
    "            full_tensor = torch.cat((full_tensor, true_tensor), dim=0)\n",
    "        n_sims = full_tensor.size(0)\n",
    "        output_tensor = full_tensor[1:n_sims]\n",
    "        out = output_tensor.unsqueeze(-1)\n",
    "        return out\n",
    "    \n",
    "num_dim = 2\n",
    "prior = utils.BoxUniform(low=10**-2 * torch.ones(num_dim), high=250 * torch.ones(num_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 600, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_data = simulator(torch.tensor(true_params).unsqueeze(0))\n",
    "true_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_wrapper, prior = prepare_for_sbi(simulator, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding net is a RNN consisting of a single layer of 100 GRU units (Cho et al, 2014)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size #Number of GRU units\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)   #Batch size is specified by the second shape number of input. \n",
    "                                                                        #In this case, batch_size = 1000 because input.shape is [1, 6000]. (https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
    "                                                                        #Weights are therefore updated after a pass through the entire trajectory\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x): #The input will be (batch_size, seq_length, inp_size)\n",
    "        x = x.view(-1, 100, 6) # have a sequence length of 100 and 6 input size\n",
    "        h = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Number of layers, batch_size and number of GRU units\n",
    "        gru_out, _ = self.gru(x, h)                # GRU layer, we don't store the hn (hidden state output)\n",
    "        output = self.linear(gru_out[:, -1, :])     #Last hidden state\n",
    "        return output\n",
    "\n",
    "# Define RNN\n",
    "num_layers = 1\n",
    "input_size = 6\n",
    "# batch_size = 100\n",
    "output_size = 25 #We want 1-dimensional embeddings of length = 25 (1x25)\n",
    "hidden_size = 100 #GRU units\n",
    "\n",
    "embedding_net_RNN = RNN(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6000, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It should be 3, 1, 6000\n",
    "batch_size = 2\n",
    "fourdim = simulator_wrapper(prior.sample((batch_size,)))\n",
    "fourdim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourdim_r = fourdim.view(-1, 1000, 6)\n",
    "fourdim_r.shape #every timepoint is a list of 6..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru = nn.GRU(6, 100, batch_first=True)\n",
    "fc = nn.Linear(100, 25)\n",
    "h = torch.zeros(1, fourdim_r.size(0), 100)  # Number of layers, batch_size and number of GRU units\n",
    "gru_out, _ = gru(fourdim_r, h)                # GRU layer, we don't store the hn (hidden state output)\n",
    "gru_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = fc(gru_out[:, -1, :])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now SNPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallelise it\n",
    "# PART 1: Edited code from original sbi repos. Functions edited: simulate_for_sbi, and all the functions it contains.\n",
    "#Main changes: defined simulator_seeded globally rather than within simulate_in_batches function, and re-imported torch within simulator_seeded \n",
    "#NO changes need to be made in this file\n",
    "\n",
    "# PART 2: Essentially the unparallelised code with 2 extra arguments in the simulate_for_sbi line\n",
    "\n",
    "##################################### PART 1 #########################################\n",
    "import joblib\n",
    "import contextlib\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import Tensor, split, randint, cat\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sbi\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sbi.inference import SNLE, prepare_for_sbi\n",
    "# No longer importing simulate_for_sbi from the package, we use the one defined above\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "\n",
    "def seed_all_backends(seed: Optional[Union[int, Tensor]] = None) -> None:\n",
    "    if seed is None:\n",
    "        seed = int(torch.randint(1_000_000, size=(1,)))\n",
    "    else:\n",
    "        # Cast Tensor to int (required by math.random since Python 3.11)\n",
    "        seed = int(seed)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False # type: ignore\n",
    "\n",
    "def simulator_seeded(simulator: Callable, theta: Tensor, seed: int) -> Tensor:\n",
    "    import torch\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    with torch.random.fork_rng(devices=[]):\n",
    "        torch.manual_seed(seed)\n",
    "        return simulator(theta)\n",
    "\n",
    "def simulate_in_batches(simulator: Callable, theta: Tensor, sim_batch_size: int = 1, num_workers: int = 1,\n",
    "                        seed: Optional[int] = None, show_progress_bars: bool = True, ) -> Tensor:\n",
    "\n",
    "    num_sims, *_ = theta.shape\n",
    "    seed_all_backends(seed)\n",
    "\n",
    "    if num_sims == 0:\n",
    "        x = torch.tensor([])\n",
    "    elif sim_batch_size is not None and sim_batch_size < num_sims:\n",
    "        batches = split(theta, sim_batch_size, dim=0)\n",
    "\n",
    "        if num_workers != 1:\n",
    "            batch_seeds = randint(high=1_000_000, size=(len(batches),))\n",
    "\n",
    "            with tqdm_joblib(\n",
    "                tqdm(batches, disable=not show_progress_bars,\n",
    "                     desc=f\"Running {num_sims} simulations in {len(batches)} batches.\", total=len(batches),)) as _:\n",
    "                simulation_outputs = Parallel(n_jobs=num_workers)(delayed(simulator_seeded)(simulator, batch, batch_seed)\n",
    "                    for batch, batch_seed in zip(batches, batch_seeds))\n",
    "        else:\n",
    "            pbar = tqdm(total=num_sims, disable=not show_progress_bars, desc=f\"Running {num_sims} simulations.\", )\n",
    "\n",
    "            with pbar:\n",
    "                simulation_outputs = []\n",
    "                for batch in batches:\n",
    "                    simulation_outputs.append(simulator_seeded(simulator, batch, seed))\n",
    "                    pbar.update(sim_batch_size)\n",
    "\n",
    "        x = cat(simulation_outputs, dim=0)\n",
    "    else:\n",
    "        x = simulator(theta)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "def simulate_for_sbi(simulator: Callable, proposal: Any, num_simulations: int, num_workers: int = 1,\n",
    "                     simulation_batch_size: int = 1, seed: Optional[int] = None,\n",
    "                     show_progress_bar: bool = True, ) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    theta = proposal.sample((num_simulations,))\n",
    "\n",
    "    x = simulate_in_batches(simulator=simulator, theta=theta, sim_batch_size=simulation_batch_size,\n",
    "                            num_workers=num_workers, seed=seed, show_progress_bars=show_progress_bar,)\n",
    "\n",
    "    return theta, x\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "simulation_batch_size = 200\n",
    "num_simulations = 1200\n",
    "\n",
    "# To parallelise, set number of CPUs to be used. Note: parallelise anything that has num_rounds>2 and/or num_simulations>50\n",
    "# to see total available CPUs: print(os.cpu_count())\n",
    "\n",
    "CPUs_to_use = 8\n",
    "\n",
    "total_CPUs = os.cpu_count()\n",
    "num_workers = CPUs_to_use - total_CPUs -1\n",
    "# num_workers = -1 uses all cpus\n",
    "# num_workers = -2 uses all cpus but one etc https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\n",
    "if CPUs_to_use > total_CPUs:\n",
    "    raise ValueError(f\"Cannot use more CPUs than are available. Available CPUs: {total_CPUs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8c5d731a2d4d8285ea762381f8d56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 1000 simulations in 7 batches.:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 114 epochs."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f5dac737ce40e7b85bdf7fa50f73e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d90d20d8a45459cbcef7ca8a7c83653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 1000 simulations in 7 batches.:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SNPE-C with atomic loss\n",
      " Training neural network. Epochs trained: 2"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rounds):\n\u001b[1;32m     21\u001b[0m     theta, x \u001b[38;5;241m=\u001b[39m simulate_for_sbi(simulator_wrapper, proposal, num_simulations\u001b[38;5;241m=\u001b[39mnum_simulations, simulation_batch_size\u001b[38;5;241m=\u001b[39msimulation_batch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[1;32m     22\u001b[0m     density_estimator \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_simulations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproposal\u001b[49m\n\u001b[0;32m---> 24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mbuild_posterior(density_estimator)\n\u001b[1;32m     26\u001b[0m     posteriors\u001b[38;5;241m.\u001b[39mappend(posterior)\n",
      "File \u001b[0;32m~/Desktop/SBI/.conda/lib/python3.11/site-packages/sbi/inference/snpe/snpe_c.py:180\u001b[0m, in \u001b[0;36mSNPE_C.train\u001b[0;34m(self, num_atoms, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, use_combined_loss, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_non_atomic_loss:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;66;03m# Take care of z-scoring, pre-compute and store prior terms.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_state_for_mog_proposal()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SBI/.conda/lib/python3.11/site-packages/sbi/inference/snpe/snpe_base.py:367\u001b[0m, in \u001b[0;36mPosteriorEstimator.train\u001b[0;34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(train_losses)\n\u001b[1;32m    365\u001b[0m train_log_probs_sum \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m train_losses\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 367\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_max_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     clip_grad_norm_(\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neural_net\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mclip_max_norm\n\u001b[1;32m    371\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/SBI/.conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SBI/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_rounds = 2\n",
    "#Reset embedding net?\n",
    "\n",
    "simulator_wrapper, prior = prepare_for_sbi(simulator, prior)\n",
    "\n",
    "\n",
    "# We define the neural network (neural density estimator), specifying the embedding net. In this case we use a mixture density network.\n",
    "neural_posterior = utils.posterior_nn(\n",
    "    model=\"maf\", embedding_net=embedding_net_RNN, hidden_features=25, num_transforms=2)\n",
    "\n",
    "# Setup the inference procedure with the SNPE-C (Greenberg et al, 2019)\n",
    "inference = SNPE(prior=prior, density_estimator=neural_posterior)\n",
    "\n",
    "posteriors = []\n",
    "proposal = prior\n",
    "\n",
    "num_simulations = 1000\n",
    "simulation_batch_size = 150\n",
    "\n",
    "for _ in range(num_rounds):\n",
    "    theta, x = simulate_for_sbi(simulator_wrapper, proposal, num_simulations=num_simulations, simulation_batch_size=simulation_batch_size, num_workers=num_workers)\n",
    "    density_estimator = inference.append_simulations(\n",
    "        theta, x, proposal=proposal\n",
    "    ).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    posteriors.append(posterior)\n",
    "    proposal = posterior.set_default_x(true_data)\n",
    "\n",
    "posterior_samples = posterior.sample((50000,), x=true_data)\n",
    "_ = analysis.pairplot(\n",
    "    posterior_samples, limits=[[-100, 300], [-100, 300], [-100, 300]], figsize=(6, 6) #It should be close to 247, but since this is a toy example we are not expecting it to work (just checking it runs).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too long to train...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallelise it\n",
    "# PART 1: Edited code from original sbi repos. Functions edited: simulate_for_sbi, and all the functions it contains.\n",
    "#Main changes: defined simulator_seeded globally rather than within simulate_in_batches function, and re-imported torch within simulator_seeded \n",
    "#NO changes need to be made in this file\n",
    "\n",
    "# PART 2: Essentially the unparallelised code with 2 extra arguments in the simulate_for_sbi line\n",
    "\n",
    "##################################### PART 1 #########################################\n",
    "import joblib\n",
    "import contextlib\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import Tensor, split, randint, cat\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sbi\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sbi.inference import SNLE, prepare_for_sbi\n",
    "# No longer importing simulate_for_sbi from the package, we use the one defined above\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "\n",
    "def seed_all_backends(seed: Optional[Union[int, Tensor]] = None) -> None:\n",
    "    if seed is None:\n",
    "        seed = int(torch.randint(1_000_000, size=(1,)))\n",
    "    else:\n",
    "        # Cast Tensor to int (required by math.random since Python 3.11)\n",
    "        seed = int(seed)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False # type: ignore\n",
    "\n",
    "def simulator_seeded(simulator: Callable, theta: Tensor, seed: int) -> Tensor:\n",
    "    import torch\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    with torch.random.fork_rng(devices=[]):\n",
    "        torch.manual_seed(seed)\n",
    "        return simulator(theta)\n",
    "\n",
    "def simulate_in_batches(simulator: Callable, theta: Tensor, sim_batch_size: int = 1, num_workers: int = 1,\n",
    "                        seed: Optional[int] = None, show_progress_bars: bool = True, ) -> Tensor:\n",
    "\n",
    "    num_sims, *_ = theta.shape\n",
    "    seed_all_backends(seed)\n",
    "\n",
    "    if num_sims == 0:\n",
    "        x = torch.tensor([])\n",
    "    elif sim_batch_size is not None and sim_batch_size < num_sims:\n",
    "        batches = split(theta, sim_batch_size, dim=0)\n",
    "\n",
    "        if num_workers != 1:\n",
    "            batch_seeds = randint(high=1_000_000, size=(len(batches),))\n",
    "\n",
    "            with tqdm_joblib(\n",
    "                tqdm(batches, disable=not show_progress_bars,\n",
    "                     desc=f\"Running {num_sims} simulations in {len(batches)} batches.\", total=len(batches),)) as _:\n",
    "                simulation_outputs = Parallel(n_jobs=num_workers)(delayed(simulator_seeded)(simulator, batch, batch_seed)\n",
    "                    for batch, batch_seed in zip(batches, batch_seeds))\n",
    "        else:\n",
    "            pbar = tqdm(total=num_sims, disable=not show_progress_bars, desc=f\"Running {num_sims} simulations.\", )\n",
    "\n",
    "            with pbar:\n",
    "                simulation_outputs = []\n",
    "                for batch in batches:\n",
    "                    simulation_outputs.append(simulator_seeded(simulator, batch, seed))\n",
    "                    pbar.update(sim_batch_size)\n",
    "\n",
    "        x = cat(simulation_outputs, dim=0)\n",
    "    else:\n",
    "        x = simulator(theta)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "def simulate_for_sbi(simulator: Callable, proposal: Any, num_simulations: int, num_workers: int = 1,\n",
    "                     simulation_batch_size: int = 1, seed: Optional[int] = None,\n",
    "                     show_progress_bar: bool = True, ) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    theta = proposal.sample((num_simulations,))\n",
    "\n",
    "    x = simulate_in_batches(simulator=simulator, theta=theta, sim_batch_size=simulation_batch_size,\n",
    "                            num_workers=num_workers, seed=seed, show_progress_bars=show_progress_bar,)\n",
    "\n",
    "    return theta, x\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "simulation_batch_size = 1500\n",
    "num_simulations = 10000\n",
    "\n",
    "# To parallelise, set number of CPUs to be used. Note: parallelise anything that has num_rounds>2 and/or num_simulations>50\n",
    "# to see total available CPUs: print(os.cpu_count())\n",
    "\n",
    "CPUs_to_use = 8\n",
    "\n",
    "total_CPUs = os.cpu_count()\n",
    "num_workers = CPUs_to_use - total_CPUs -1\n",
    "# num_workers = -1 uses all cpus\n",
    "# num_workers = -2 uses all cpus but one etc https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\n",
    "if CPUs_to_use > total_CPUs:\n",
    "    raise ValueError(f\"Cannot use more CPUs than are available. Available CPUs: {total_CPUs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c24c03c1042484abade83c6f2657d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 10000 simulations in 7 batches.:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 132 epochs."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1201e9387f74dd69db4cbed15bcd4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 10000 posterior samples:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAIgCAYAAAAcFrryAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlUElEQVR4nO3df2zV9d34/VfBFgulFJCCwlAuu+gMioYpqVvucU2+4OKv/cjm7cxXw8ycDq97OC4yzaLkynIFN+fCZozzyvZFc20ZxkRmpnO3TLRu32EVbL+KTDa8cSxV5FIpP2qhpX3ff3j1jArIr0NPT9+PR9LQc86n7fttj5wnn8/nvD8VKaUUAEB2hpV6AABAaYgAAMiUCACATIkAAMiUCACATIkAAMiUCACATIkAAMiUCACATJ1U6gEAQ9P/GPblUg8BhoxVvY+ckO9rTwAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmLBbEoNHW3hnbO7oiImLsqKqYXFdd4hEBDG0igEGhrb0z5tzTFJ3dPRERUV05PH6/6DNCAOAEcjiAQWF7R1d0dvfEsqvPj2VXnx+d3T2FvQIAnBj2BDCoNNTXlHoIANmwJwAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBT3iLICde3EqBVAAEGFxHACbX/SoBWAQQYXBwO4ITqWwnwXz7bcNBVANvaO2N9247YtG13iUYIkC97AhgQB/vX/8GuFzB2VJXlggEGiAigZPa/XkBDfU3hnAERADAwRAAl11BfE9Mnjyn1MACyIwIA4BBO+tiUfre7Pzb+gG0q/vR/Bmo4RefEQADIlD0BDGp9awxEhHUGAIpMBDBotbV3xpd/uqbfuwesMwBQPCKAQWv/dw9ERCx8uDW2d3SJAGDgpNTvZseUA//+qRmosZwAIoBBr6G+nP8XAxi8nBgIAJkSAQCQKREAAJlyTgAAHKG9YyoOuK+cz1oSAQyovqsFjh1VVeKRACACGBBjR1VFdeXwWPhwa0R88J7/Zf/3+SUdE0DuRAADYnJddfx+0Wdie0dXbNq2u/CefwBKRwQwYCbXVR/3Qj/7H06waBBwwn1osaBdpx+4yYGXFCofIoBBq629s/D5wQ4nWEIY4PiIAEpm/xf5/fW94N+7elNUVw4v/Kv/YIcTRADAsRMBDLiDvcjvb/8X/P13+xfjcAIA/yACGHCHepH/8DZe8IGSq+z/MrlvZDrEhuVJBFASXuQBSs+ywQCQKREAAJkSAQCQKecEAMCh7O2/smnF0Dov0J4AAMiVCACATIkAAMiUcwIoW5u27XYhIWBAnfSxjlIPoajsCaDs7H8xoTn3NB3yGgQAfDQRQNnpW3Z42dXnR2d3T2zv6Dr8FwFwAIcDKEuT66pje31NqYcBUNZEAAAcyskj+t0cefLQ2vPocAAAZEoEAECmRAAAZMo5AQBwKPt6+t2cUf/mAZsceE/5sCcAADJlTwAnRFt7Z2zv6IpN23aXeigAHIIIoOja2jtjzj1N0dn9wW606srhMXZUVYlHBcCHiQCKbntHV3R298Syq8+Phvoa6/sDDFIigBOmob4mpk8eU+phAByz1NV/caDLx/2fA7b5j/ingRpO0TkxEAAyJQIAIFMOBzBk9L0jISKchwBwBEQAQ8LB3pHw+0WfEQJAUU04aWeph1BUIoCy17cWQd87EiIiFj7cGts7ukQAwEcQAZStsaOqorpyeCx8uDUiPvjX/4XTxhUOCQDw0UQAZWtyXXX8ftFnDjgPQAQAHBkRQFmbXFdtlz9wwqT3O/vd/r9OPnCbfx+gsZwI3iIIAJkSAQCQKREAAJlyTgBF4/LBAOVFBFAULh8MDEnd3f1uvty1p0QDOTFEAEXh8sEA5UcEUFQuHwxQPpwYCACZsicAAI7QpOE9pR5CUdkTAACZEgEAkCkRwJC1advuaGvvPPyGAJlyTgBDzv6XGK6uHB6/X/QZb1cEjknv3r39bn/3zbkH2apjYAZzAtgTwJDTd4nhZVefH53dPS4tDHAI9gQwJE2uq47t9TWlHgbAoGZPAABkSgQAQKYcDgCAQ0mp380X3pp6wCaT4s8DNZqisycAADIlAjhube2dsWnb7lIPA4Cj5HAAx6WtvTPm3NMUnd09UV05PMaOqir1kA7QFygubwzQnwjguGzv6IrO7p5YdvX5ceG0cYPqRXb/RYMiwsJBwHHr/d9jSz2EohIBFEVDfc2ge3HtWzRoe0dXbNq2OxY+3BrbO7oG3TgBSkUEMKRNrqv2og9wCE4MBIBM2RMAAEdo/PruUg+hqOwJAIBMiQAAyJQIICubtu2OtvbOUg8DYFAQAWRh/zUD5tzTJAQAwomBZKJvzYAXN79nvQDgmI188f874L6eEoyjWEQA2ZhcVx3b62siIg56rQPLCgO5EQFk5cNLCe/PssJAbkQAWdl/KeH9WVYYyJEIIDuWEgaOVe+uoXXZdO8OAIBMiQAAyJQIAIBMOScAAI5Q2ru31EMoKnsCACBTIgAAMiUCYD8uMATkRARAuMAQkCcRAPGPlQSXXX1+dHb3HLCiIMBQ5N0B8N/2v8AQQA7sCQCATIkAAMiUCACATIkAAMiUCACATIkAAMiUCACATIkAAMiUCACATFkxEA5i07bdEfHBNQUm11WXeDQAJ4YIgP3sfyGhiIjqyuHx+0WfEQLAkCQCOCZt7Z2xvaOr8C/moaLvQkJ9c1v4cGts7+gSAcCQJAI4am3tnTHnnqbo7O6JiA/+tTx2VFWJR1U8k+uqvegDWRABHLXtHV3R2d0Ty64+Pxrqaxw3ByhTIoBj1lBfE9Mnjyn1MAA4Rt4iCACZEgEAkCmHA+AI9b0jwjkQwFAhAuAwNm3bHe92dMVN/7kuOrt7rB0ADBkOB8Ah7L9w0PX/64WIiLjz8nOis7sntnd0lXh0AMfPngA4hP0XDor4IAq8+ANDiQiAj/DhhYNEADCUOBwAAJkSAQCQKREAAJkSAQCQKREAAJny7gCOWN+KeZu27S71UAAoAhHAEWlr74w59zRFZ3dPRERUVw6PsaOqSjwqAI6HCOCIbO/ois7unlh29fnRUF9j/XyAIUAEcFQa6mti+uQxpR4GAEUgAuA49J0nERH2jgBlRwTAMTrYeRKuLgiUE28RhGO0/3kSy64+39UFgbJjTwAcp4b6mlIPAeCY2BMAAJmyJwCOgQWTgKFABMBRGDuqKqorh8fCh1sj4h+LJjkXAChHIgCOwuS66vj9os8c8LZAEQCUIxEAR2lyXbW3AQJDghMDASBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIoDDamvvdOlcgCHIBYT4SG3tnTHnnqbo7O4pXDYXgKFBBPCRtnd0RWd3Tyy7+vy4cNo4V88DGEIcDuCINNTXCACAIUYEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZOqkUg+AwamtvTO2d3TFpm27Sz0UAE4QEcAB2to7Y849TdHZ3RMREdWVw2PsqKoSjwqAYhMBHGB7R1d0dvfEsqvPj4b6mhg7qiom11WXelgAFJkI4JAa6mti+uQxpR4GACeIEwMBIFMiAAAyJQIAIFMiAAAyJQIAIFMiAAAyJQIAIFMigH7a2jstFQyQCYsFUbD/csGWCgYY+kQABfsvF3zhtHGWCgYY4hwO4AAN9TUCACADIgAAMuVwANHW3hnbO7qcEAiQGRGQuf1PBowIJwQCZEQEZG7/kwEb6mti7Kgq5wMAZEIEEBEfnAw4ffKYUg8DgAEkAo5D37H0cvzXs/MAABABx+jDC+v89H/OjPFlciz93Y6uuOk/1zkPACBzFSmlVOpBAAADzzoBAJApEQAAmRIBAJApEQAAmRIBAJCpAXmLYEopdu3aNRA/CrIwevToqKioKPUwgDI3IBHwzjvvRH19/UD8KMjCtm3bYsKECaUeBlDmBiQCqqo+WIjm73//e9TW1g7Ejzyhdu7cGR/72MeGzHwizKlc9M2p7/8pgOMxIBHQt9uytrZ2yPxlHDH05hNhTuXCoQCgGJwYCACZEgEAkKkBiYARI0bEkiVLYsSIEQPx4064oTafCHMqF0NxTkDpuIAQAGTK4QAAyJQIAIBMiQAAyJQIAIBMHXcEPProozF37twYP358VFRURGtr6wHb7NmzJxYsWBDjx4+Pmpqa+NKXvhRvv/12v222bNkSl112WYwcOTLq6+tj8eLFsW/fvuMd3glx3333xRlnnBEnn3xyzJo1K1544YVSD+mQnnvuubjiiivitNNOi4qKivj1r3/d7/GUUtx5551x6qmnRnV1dcyZMyf++te/9tvmvffei2uvvTZqa2ujrq4ubrjhhti9e/cAzuIfli5dGhdeeGGMHj066uvr4/Of/3xs3Lix3zbl9ny7//7747zzzissatTY2BhPPvlk4fFymw9QPo47Ajo6OuLTn/50fP/73z/kNrfeemv85je/iUceeSSamprizTffjC9+8YuFx3t6euKyyy6Lrq6u+NOf/hQPPfRQPPjgg3HnnXce7/CK7uGHH45vf/vbsWTJknjppZdixowZMW/evNi2bVuph3ZQHR0dMWPGjLjvvvsO+vgPfvCD+MlPfhI//elPo7m5OUaNGhXz5s2LPXv2FLa59tpr49VXX41Vq1bF448/Hs8991zceOONAzWFfpqammLBggXx/PPPx6pVq6K7uzvmzp0bHR0dhW3K7fk2ZcqUuOuuu2LdunWxdu3a+OxnPxtXXXVVvPrqq2U5H6CMpCLZvHlziojU0tLS7/729vZUWVmZHnnkkcJ9f/7zn1NEpDVr1qSUUvrtb3+bhg0blrZu3VrY5v7770+1tbVp7969xRpiUVx00UVpwYIFhds9PT3ptNNOS0uXLi3hqI5MRKSVK1cWbvf29qZJkyalu+++u3Bfe3t7GjFiRPrVr36VUkppw4YNKSLSiy++WNjmySefTBUVFamtrW3Axn4o27ZtSxGRmpqaUkpD5/k2duzY9LOf/WzIzAcYnE74OQHr1q2L7u7umDNnTuG+s88+O6ZOnRpr1qyJiIg1a9bEueeeGxMnTixsM2/evNi5c2fhX0ODQVdXV6xbt67fXIYNGxZz5swpzKWcbN68ObZu3dpvPmPGjIlZs2b1+93U1dXFJz/5ycI2c+bMiWHDhkVzc/OAj/nDduzYERER48aNi4jyf7719PTEihUroqOjIxobG8t+PsDgdsIvILR169aoqqqKurq6fvdPnDgxtm7dWthm/7/A+h7ve2yweOedd6Knp+egY33ttddKNKpj1/ff9mDz2f938+HLQJ900kkxbty4kv9uent7Y+HChfGpT30qpk+fHhHl+3x75ZVXorGxMfbs2RM1NTWxcuXKOOecc6K1tbUs5wOUh6PaE/DLX/4yampqCh9/+MMfTtS44LAWLFgQ69evjxUrVpR6KMftrLPOitbW1mhubo6bb745rr/++tiwYUOphwUMcUcVAVdeeWW0trYWPvbfRXwokyZNiq6urmhvb+93/9tvvx2TJk0qbPPhs537bvdtMxiccsopMXz48IOOdTCN80j1jfmj5jNp0qQDTnrct29fvPfeeyWd8y233BKPP/54PPPMMzFlypTC/eX6fKuqqoqGhoaYOXNmLF26NGbMmBE//vGPy3Y+QHk4qggYPXp0NDQ0FD6qq6sP+zUzZ86MysrKePrppwv3bdy4MbZs2RKNjY0REdHY2BivvPJKvxebVatWRW1tbZxzzjlHM8QTqqqqKmbOnNlvLr29vfH0008X5lJOpk2bFpMmTeo3n507d0Zzc3O/3017e3usW7eusM3q1aujt7c3Zs2aNeBjTinFLbfcEitXrozVq1fHtGnT+j0+VJ5vvb29sXfv3iEzH2CQOt4zC999993U0tKSnnjiiRQRacWKFamlpSW99dZbhW1uuummNHXq1LR69eq0du3a1NjYmBobGwuP79u3L02fPj3NnTs3tba2pt/97ndpwoQJ6fbbbz/e4RXdihUr0ogRI9KDDz6YNmzYkG688cZUV1fX78zswWTXrl2ppaUltbS0pIhIP/rRj1JLS0v629/+llJK6a677kp1dXXpscceSy+//HK66qqr0rRp01JnZ2fhe1x66aXpggsuSM3NzemPf/xj+vjHP56uueaakszn5ptvTmPGjEnPPvtseuuttwof77//fmGbcnu+3XbbbampqSlt3rw5vfzyy+m2225LFRUV6amnnirL+QDl47gjYPny5SkiDvhYsmRJYZvOzs70zW9+M40dOzaNHDkyfeELX+gXCSml9MYbb6TPfe5zqbq6Op1yyilp0aJFqbu7+3iHd0Lce++9aerUqamqqipddNFF6fnnny/1kA7pmWeeOejv5/rrr08pffA2wTvuuCNNnDgxjRgxIl1yySVp48aN/b7Hu+++m6655ppUU1OTamtr0/z589OuXbtKMJt00LlERFq+fHlhm3J7vn3ta19Lp59+eqqqqkoTJkxIl1xySSEAUiq/+QDlw6WEASBTrh0AAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREwRM2ePTsWLlxYuH3GGWfEsmXLSjYeAAafk0o9AAbGiy++GKNGjSr69/33f//3eOKJJwrXvf/w1e4AGLzsCcjEhAkTYuTIkUX/vl1dXfHlL385br755qJ/bwBOLBEwBHR0dMR1110XNTU1ceqpp8Y999xzwDYfPhxQUVERDzzwQFx++eUxcuTI+MQnPhFr1qyJTZs2xezZs2PUqFFx8cUXx+uvv/6RP/vf/u3f4tZbb41zzz232NMC4AQTAUPA4sWLo6mpKR577LF46qmn4tlnn42XXnrpsF/3ve99L6677rpobW2Ns88+O7761a/GN77xjbj99ttj7dq1kVKKW265ZQBmAEApOCegzO3evTt+/vOfxy9+8Yu45JJLIiLioYceiilTphz2a+fPnx9f+cpXIiLiO9/5TjQ2NsYdd9wR8+bNi4iIb33rWzF//vwTN3gASsqegDL3+uuvR1dXV8yaNatw37hx4+Kss8467Need955hc8nTpwYEdFvt/7EiRNjz549sXPnziKOGIDBQgRkrLKysvB5RUXFIe/r7e0d2IEBMCBEQJk788wzo7KyMpqbmwv3bd++Pf7yl7+UcFQAlAPnBJS5mpqauOGGG2Lx4sUxfvz4qK+vj+9+97sxbNjA9N2WLVvivffeiy1btkRPT0+0trZGRERDQ0PU1NQMyBgAODYiYAi4++67Y/fu3XHFFVfE6NGjY9GiRbFjx44B+dl33nlnPPTQQ4XbF1xwQUREPPPMMzF79uwBGQMAx6YipZRKPQgAYOA5JwAAMiUCACBTIgAAMiUCACBTIgAAMiUCAOAorG/bEWfc9kSsbxuYt2KfSCIAAI5C34u/CAAAypYIAICjsP7NHf3+LGciAACOwvq2Dy6v/tSrb5d4JMdPBADAUXizvTMiIrbt2htt//15uRIBAHAUtr/f9Y/PO7o+YsvBTwQAQKZEAABkSgQAQKZEAAAco3JfMEgEAMARWt+2I7p70j9ul/laASIAAI5Quf/L/8NEAABkSgQAQKZEAABkSgQAQKZEAABkSgQAwDGaftqYUg/huIgAADhG0yeLAACgDIkAAMiUCACATIkAAMiUCACATIkAADhCY0dVlXoIRSUCAOAITa6rjoiI/+ezDSUeSXGIAAA4Sqf9dwyUOxEAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAJkSAQCQKREAAEdgfduOuPzeP5Z6GEUlAgDgCKxv21HqIRSdCACATIkAAMiUCACATIkAAMiUCACATIkAAMiUCACATIkAADgCY0dVFT5/s72zhCMpHhEAAEdgcl114fP33u8q4UiKRwQAQKZEAABkSgQAQKZEAAAcpemnjSn1EIripFIPAADKyeP/8ulSD6Fo7AkAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgCPQ1t5Z6iEUnQgAgMNoa++Mb/znusLnQ4UIAIDD2N7RVfh8cl11CUdSXCIAAI7Q4//y6Zg+eUyph1E0IgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAAMiUCACBTIgAADmN9245+fw4VIgAADmP9mzv6/TlUiAAAOIzpp43p9+dQIQIA4DCmTx7T78+hQgQAQKZEAABkSgQAQKZEAAAcxrsdXaUewgkhAgDgI7S1d8ZN/7kuqiuHx9hRVaUeTlGdVOoBAMBgtr2jKzq7e+Khr10Uk+uqSz2corInAACOwPghthcgQgQAQLZEAABkSgQAwEcYqu8MiBABAHBI/++rW+P6//VCRMSQe2dARERFSimVehAAwMCzJwAAMiUCACBTIgAAMiUCACBTIgAAMuXaAUDRpZRi165dpR4GDCmjR4+OioqKon5PEQAU3TvvvBP19fWlHgYMKdu2bYsJEyYU9XuKAKDoqqo+WFTl73//e9TW1pZ4NMWxc+fO+NjHPmZOg9xQnlPf/1fFJAKAouvbZVlbWztk/iLuY07lYSjOqdiHAiKcGAgA2RIBAJApEQAU3YgRI2LJkiUxYsSIUg+laMypPJjT0XEBIQDIlD0BAJApEQAAmRIBAJApEQAAmRIBwFF59NFHY+7cuTF+/PioqKiI1tbWA7bZs2dPLFiwIMaPHx81NTXxpS99Kd5+++1+22zZsiUuu+yyGDlyZNTX18fixYtj3759AzSLo3fffffFGWecESeffHLMmjUrXnjhhVIP6aCee+65uOKKK+K0006LioqK+PWvf93v8ZRS3HnnnXHqqadGdXV1zJkzJ/7617/22+a9996La6+9Nmpra6Ouri5uuOGG2L179wDOor+lS5fGhRdeGKNHj476+vr4/Oc/Hxs3buy3Tbk95+6///4477zzCosaNTY2xpNPPll4fKDmIwKAo9LR0RGf/vSn4/vf//4ht7n11lvjN7/5TTzyyCPR1NQUb775Znzxi18sPN7T0xOXXXZZdHV1xZ/+9Kd46KGH4sEHH4w777xzIKZw1B5++OH49re/HUuWLImXXnopZsyYEfPmzYtt27aVemgH6OjoiBkzZsR999130Md/8IMfxE9+8pP46U9/Gs3NzTFq1KiYN29e7Nmzp7DNtddeG6+++mqsWrUqHn/88XjuuefixhtvHKgpHKCpqSkWLFgQzz//fKxatSq6u7tj7ty50dHRUdim3J5zU6ZMibvuuivWrVsXa9eujc9+9rNx1VVXxauvvjqw80kAx2Dz5s0pIlJLS0u/+9vb21NlZWV65JFHCvf9+c9/ThGR1qxZk1JK6be//W0aNmxY2rp1a2Gb+++/P9XW1qa9e/cOyPiPxkUXXZQWLFhQuN3T05NOO+20tHTp0hKO6vAiIq1cubJwu7e3N02aNCndfffdhfva29vTiBEj0q9+9auUUkobNmxIEZFefPHFwjZPPvlkqqioSG1tbQM29o+ybdu2FBGpqakppTR0nnNjx45NP/vZzwZ0PvYEAEW1bt266O7ujjlz5hTuO/vss2Pq1KmxZs2aiIhYs2ZNnHvuuTFx4sTCNvPmzYudO3cW/iU0WHR1dcW6dev6zWfYsGExZ86cwnzKxebNm2Pr1q395jJmzJiYNWtWv99NXV1dfPKTnyxsM2fOnBg2bFg0NzcP+JgPZseOHRERMW7cuIgo/+dcT09PrFixIjo6OqKxsXFA5+MCQkBRbd26NaqqqqKurq7f/RMnToytW7cWttn/L6++x/seG0zeeeed6OnpOeh4X3vttRKN6tj0/bc92Fz2/918+DLQJ510UowbN25Q/G56e3tj4cKF8alPfSqmT58eEeX7nHvllVeisbEx9uzZEzU1NbFy5co455xzorW1dcDmY08AcEi//OUvo6ampvDxhz/8odRDInMLFiyI9evXx4oVK0o9lON21llnRWtrazQ3N8fNN98c119/fWzYsGFAxyACgEO68soro7W1tfCx/y7iQ5k0aVJ0dXVFe3t7v/vffvvtmDRpUmGbD5/p3He7b5vB4pRTTonhw4cfdLyDbayH0zfej5rLpEmTDjjhcd++ffHee++VfL633HJLPP744/HMM8/ElClTCveX63OuqqoqGhoaYubMmbF06dKYMWNG/PjHPx7Q+YgA4JBGjx4dDQ0NhY/q6urDfs3MmTOjsrIynn766cJ9GzdujC1btkRjY2NERDQ2NsYrr7zS78Vm1apVUVtbG+ecc07xJ3IcqqqqYubMmf3m09vbG08//XRhPuVi2rRpMWnSpH5z2blzZzQ3N/f73bS3t8e6desK26xevTp6e3tj1qxZAz7miA/e1njLLbfEypUrY/Xq1TFt2rR+jw+V51xvb2/s3bt3YOdTtNMagSy8++67qaWlJT3xxBMpItKKFStSS0tLeuuttwrb3HTTTWnq1Klp9erVae3atamxsTE1NjYWHt+3b1+aPn16mjt3bmptbU2/+93v0oQJE9Ltt99eiikd1ooVK9KIESPSgw8+mDZs2JBuvPHGVFdX1+/M7MFi165dqaWlJbW0tKSISD/60Y9SS0tL+tvf/pZSSumuu+5KdXV16bHHHksvv/xyuuqqq9K0adNSZ2dn4Xtceuml6YILLkjNzc3pj3/8Y/r4xz+errnmmlJNKd18881pzJgx6dlnn01vvfVW4eP9998vbFNuz7nbbrstNTU1pc2bN6eXX3453XbbbamioiI99dRTAzofEQAcleXLl6eIOOBjyZIlhW06OzvTN7/5zTR27Ng0cuTI9IUvfKFfJKSU0htvvJE+97nPperq6nTKKaekRYsWpe7u7gGezZG7995709SpU1NVVVW66KKL0vPPP1/qIR3UM888c9Dfz/XXX59S+uBtgnfccUeaOHFiGjFiRLrkkkvSxo0b+32Pd999N11zzTWppqYm1dbWpvnz56ddu3aVYDYfONh8IiItX768sE25Pee+9rWvpdNPPz1VVVWlCRMmpEsuuaQQACkN3HxcShgAMuWcAADIlAgAgEyJAADIlAgAgEyJAADIlAgAgEyJAADIlAgAGAJmz54dCxcuLNw+44wzYtmyZSUbD+VBBAAMQS+++GLceOONRf2eb7zxRtxwww0xbdq0qK6ujjPPPDOWLFkSXV1dRf05DJyTSj0AAIpvwoQJRf+er732WvT29sYDDzwQDQ0NsX79+vj6178eHR0d8cMf/rDoP48Tz54AgDLT0dER1113XdTU1MSpp54a99xzzwHbfPhwQEVFRTzwwANx+eWXx8iRI+MTn/hErFmzJjZt2hSzZ8+OUaNGxcUXXxyvv/76IX/upZdeGsuXL4+5c+fGP/3TP8WVV14Z//qv/xqPPvroiZgmA0AEAJSZxYsXR1NTUzz22GPx1FNPxbPPPhsvvfTSYb/ue9/7Xlx33XXR2toaZ599dnz1q1+Nb3zjG3H77bfH2rVrC5fsPRo7duyIcePGHetUKDGHAwDKyO7du+PnP/95/OIXv4hLLrkkIiIeeuihmDJlymG/dv78+fGVr3wlIiK+853vRGNjY9xxxx0xb968iIj41re+FfPnzz/isWzatCnuvfdehwLKmD0BAGXk9ddfj66urpg1a1bhvnHjxsVZZ5112K8977zzCp9PnDgxIiLOPffcfvft2bMndu7cedjv1dbWFpdeeml8+ctfjq9//etHMwUGEREAkInKysrC5xUVFYe8r7e39yO/z5tvvhn//M//HBdffHH8x3/8xwkYKQNFBACUkTPPPDMqKyujubm5cN/27dvjL3/5y4D8/La2tpg9e3bMnDkzli9fHsOGeRkpZ84JACgjNTU1ccMNN8TixYtj/PjxUV9fH9/97ncH5MW4LwBOP/30+OEPfxj/9V//VXhs0qRJJ/znU3wiAKDM3H333bF79+644oorYvTo0bFo0aLYsWPHCf+5q1atik2bNsWmTZsOOBExpXTCfz7FV5H85gAgSw7mAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZEoEAECmRAAAZOr/B44MA+vRsqo0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#just NPE\n",
    "# The prepare for sbi function checks simulator and prior are compatible with the NPE algorithm (and makes sure it adds a batch dimension)\n",
    "simulator_wrapper, prior = prepare_for_sbi(simulator, prior)\n",
    "\n",
    "# We define the neural network (neural density estimator), specifying the embedding net. In this case we use a mixture density network.\n",
    "neural_posterior = utils.posterior_nn(\n",
    "    model=\"maf\", embedding_net=embedding_net_RNN, hidden_features=25, num_transforms=2)\n",
    "\n",
    "# Setup the inference procedure with the SNPE-C (Greenberg et al, 2019)\n",
    "inference = SNPE(prior=prior, density_estimator=neural_posterior)\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=num_simulations, simulation_batch_size=simulation_batch_size, num_workers=num_workers) # Simulate data\n",
    "density_estimator = inference.append_simulations(theta, x).train()        # Train density estimator\n",
    "posterior = inference.build_posterior(density_estimator)                  # Build posterior\n",
    "\n",
    "# Then sample the posterior and plot\n",
    "posterior_samples = posterior.sample((10000,), x=true_data)\n",
    "_ = analysis.pairplot(\n",
    "    posterior_samples, limits=[[-100, 300], [-100, 300]], figsize=(6, 6) #It should be close to 247, but since this is a toy example we are not expecting it to work (just checking it runs).\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
