{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3c3e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs (torch.cuda.device_count()): 0\n",
      "Number of CPUs (multiprocessing.cpu_count()): 8\n",
      "Number of CPUs (os.cpu_count()): 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "def get_gpu_count():\n",
    "    return torch.cuda.device_count()\n",
    "\n",
    "def get_cpu_count():\n",
    "    return multiprocessing.cpu_count()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_gpus = get_gpu_count()\n",
    "    num_cpus = get_cpu_count()\n",
    "    print(\"Number of GPUs (torch.cuda.device_count()):\", num_gpus)\n",
    "    print(\"Number of CPUs (multiprocessing.cpu_count()):\", num_cpus)\n",
    "\n",
    "print(\"Number of CPUs (os.cpu_count()):\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f107ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import math\n",
    "from math import ceil\n",
    "import random\n",
    "import peakutils\n",
    "import pandas as pd\n",
    "import contextlib\n",
    "import torch\n",
    "import sys\n",
    "import sbi\n",
    "import numpy as np\n",
    "from numpy import fft, ndarray\n",
    "from scipy.integrate import odeint\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from pyro.infer.mcmc.api import MCMC\n",
    "from warnings import warn\n",
    "from torch import Tensor, split, randint, cat\n",
    "from typing import Any, Callable, Optional, Tuple, Union, Dict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pyro.infer.mcmc import HMC, NUTS\n",
    "from sbi.inference import prepare_for_sbi, SNLE\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.types import Shape, TorchTransform\n",
    "from sbi.utils.get_nn_models import (likelihood_nn,)\n",
    "from sbi.samplers.mcmc import SliceSamplerVectorized\n",
    "from sbi.samplers.mcmc.slice_numpy import MCMCSampler\n",
    "from sbi.utils import tensor2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b09aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all_backends(seed: Optional[Union[int, Tensor]] = None) -> None:\n",
    "    if seed is None:\n",
    "        seed = int(torch.randint(10_000_000, size=(1,)))\n",
    "    else:\n",
    "        # Cast Tensor to int (required by math.random since Python 3.11)\n",
    "        seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False # type: ignore\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "    \n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "    \n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "class SliceSampler(MCMCSampler):\n",
    "    def __init__(self, x, lp_f, max_width=float(\"inf\"), init_width: Union[float, np.ndarray] = 0.05, thin=None, tuning: int = 50, verbose: bool = False,):\n",
    "        MCMCSampler.__init__(self, x, lp_f, thin, verbose=verbose)\n",
    "        self.max_width = max_width\n",
    "        self.init_width = init_width\n",
    "        self.width = None\n",
    "        self.tuning = tuning\n",
    "        \n",
    "    def _tune_bracket_width(self, rng):\n",
    "        order = list(range(self.n_dims))\n",
    "        x = self.x.copy()\n",
    "\n",
    "        self.width = np.full(self.n_dims, self.init_width)\n",
    "\n",
    "        tbar = trange(self.tuning, miniters=2, disable=not self.verbose)\n",
    "        tbar.set_description(\"Tuning bracket width...\")\n",
    "        for n in tbar:\n",
    "            # for n in range(int(self.tuning)):\n",
    "            rng.shuffle(order)\n",
    "            for i in range(self.n_dims):\n",
    "                x[i], wi = self._sample_from_conditional(i, x[i], rng)\n",
    "                self.width[i] += (wi - self.width[i]) / (n + 1)\n",
    "\n",
    "    def _sample_from_conditional(self, i: int, cxi, rng):\n",
    "        assert self.width is not None, \"Chain not initialized.\"\n",
    "\n",
    "        # conditional log prob\n",
    "        Li = lambda t: self.lp_f(np.concatenate([self.x[:i], [t], self.x[i + 1 :]]))\n",
    "        wi = self.width[i]\n",
    "\n",
    "        # sample a slice uniformly\n",
    "        logu = Li(cxi) + np.log(1.0 - rng.rand())\n",
    "\n",
    "        # position the bracket randomly around the current sample\n",
    "        lx = cxi - wi * rng.rand()\n",
    "        ux = lx + wi\n",
    "        \n",
    "        # find lower bracket end\n",
    "        while Li(lx) >= logu and cxi - lx < self.max_width:\n",
    "            lx -= wi\n",
    "\n",
    "        # find upper bracket end\n",
    "        while Li(ux) >= logu and ux - cxi < self.max_width:\n",
    "            ux += wi\n",
    "\n",
    "        # sample uniformly from bracket\n",
    "        xi = (ux - lx) * rng.rand() + lx\n",
    "\n",
    "        # if outside slice, reject sample and shrink bracket\n",
    "        while Li(xi) < logu:\n",
    "            if xi < cxi:\n",
    "                lx = xi\n",
    "            else:\n",
    "                ux = xi\n",
    "            xi = (ux - lx) * rng.rand() + lx\n",
    "       \n",
    "        return xi, ux - lx\n",
    "      \n",
    "def run_fun(SliceSamplerSerial, num_samples, inits, seed, log_prob_fn: Callable, thin: Optional[int] = None, tuning: int = 50, verbose: bool = True, init_width: Union[float, np.ndarray] = 0.01,\n",
    "            max_width: float = float(\"inf\"), num_workers: int = 1, rng=np.random, show_info: bool = False, logger=sys.stdout) -> np.ndarray:\n",
    "    np.random.seed(seed)\n",
    "    posterior_sampler = SliceSampler(inits, lp_f=log_prob_fn, max_width=max_width, init_width=init_width, thin=thin, tuning=tuning, verbose=num_workers == 1 and verbose,)\n",
    "    \n",
    "    assert num_samples >= 0, \"number of samples can't be negative\"\n",
    "\n",
    "    order = list(range(posterior_sampler.n_dims))\n",
    "    L_trace = []\n",
    "    samples = np.empty([int(num_samples), int(posterior_sampler.n_dims)])\n",
    "    logger = open(os.devnull, \"w\") if logger is None else logger\n",
    "\n",
    "    if posterior_sampler.width is None:\n",
    "        # logger.write('tuning bracket width...\\n')\n",
    "        posterior_sampler._tune_bracket_width(rng)\n",
    "\n",
    "    tbar = trange(int(num_samples), miniters=10, disable=not posterior_sampler.verbose)\n",
    "    tbar.set_description(\"Generating samples\")\n",
    "    for n in tbar:\n",
    "        # for n in range(int(n_samples)):\n",
    "        for _ in range(posterior_sampler.thin):\n",
    "            rng.shuffle(order)\n",
    "\n",
    "            for i in order:\n",
    "                posterior_sampler.x[i], _ = posterior_sampler._sample_from_conditional(i, posterior_sampler.x[i], rng)\n",
    "\n",
    "        samples[n] = posterior_sampler.x.copy()\n",
    "\n",
    "        posterior_sampler.L = posterior_sampler.lp_f(posterior_sampler.x)\n",
    "        # logger.write('sample = {0}, log prob = {1:.2}\\n'.format(n+1, self.L))\n",
    "\n",
    "        if show_info:\n",
    "            L_trace.append(posterior_sampler.L)\n",
    "\n",
    "    # show trace plot\n",
    "    if show_info:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.plot(L_trace)\n",
    "        ax.set_ylabel(\"log probability\")\n",
    "        ax.set_xlabel(\"samples\")\n",
    "        plt.show(block=False)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def run(SliceSamplerSerial, log_prob_fn: Callable, num_samples: int, init_params: np.ndarray, num_chains: int = 1, thin: Optional[int] = None, verbose: bool = True, num_workers: int = 1,) -> np.ndarray:\n",
    "    num_chains , dim_samples = init_params.shape\n",
    "    # Generate seeds for workers from current random state.\n",
    "    seeds = torch.randint(high=1_000_000, size=(num_chains,))\n",
    "    for seed in seeds:\n",
    "        seed_all_backends(seed)\n",
    "    with tqdm_joblib(tqdm(range(num_chains), disable=not verbose, desc=f\"\"\"Running {num_chains} MCMC chains with {num_workers} worker{\"s\" if num_workers>1 else \"\"}.\"\"\", total=num_chains,)):\n",
    "        all_samples = Parallel(n_jobs=num_workers)(delayed(run_fun)(SliceSamplerSerial, num_samples, initial_params_batch, seed, log_prob_fn)for initial_params_batch, seed in zip(init_params, seeds))\n",
    "    samples = np.stack(all_samples).astype(np.float32)\n",
    "    samples = samples.reshape(num_chains, -1, dim_samples)  # chains, samples, dim\n",
    "    samples = samples[:, :: thin, :]  # thin chains\n",
    "\n",
    "    # save samples\n",
    "    return samples\n",
    "\n",
    "class SliceSamplerSerial:\n",
    "    def __init__(self, log_prob_fn: Callable, init_params: np.ndarray, num_chains: int = 1, thin: Optional[int] = None, tuning: int = 50, verbose: bool = True, init_width: Union[float, np.ndarray] = 0.01, max_width: float = float(\"inf\"), num_workers: int = 1,):\n",
    "        self._log_prob_fn = log_prob_fn\n",
    "        self.x = init_params\n",
    "        self.num_chains = num_chains\n",
    "        self.thin = thin\n",
    "        self.tuning = tuning\n",
    "        self.verbose = verbose\n",
    "        self.init_width = init_width\n",
    "        self.max_width = max_width\n",
    "        self.n_dims = self.x.size\n",
    "        self.num_workers = num_workers\n",
    "        self._samples = None\n",
    "\n",
    "    def get_samples(self, num_samples: Optional[int] = None, group_by_chain: bool = True) -> np.ndarray:\n",
    "        if self._samples is None:\n",
    "            raise ValueError(\"No samples found from MCMC run.\")\n",
    "        # if not grouped by chain, flatten samples into (all_samples, dim_params)\n",
    "        if not group_by_chain:\n",
    "            samples = self._samples.reshape(-1, self._samples.shape[2])\n",
    "        else:\n",
    "            samples = self._samples\n",
    "\n",
    "        # if not specified return all samples\n",
    "        if num_samples is None:\n",
    "            return samples\n",
    "        # otherwise return last num_samples (for each chain when grouped).\n",
    "        elif group_by_chain:\n",
    "            return samples[:, -num_samples:, :]\n",
    "        else:\n",
    "            return samples[-num_samples:, :]\n",
    "\n",
    "##############################################################################################################################\n",
    "        \n",
    "def _maybe_use_dict_entry(default: Any, key: str, dict_to_check: Dict) -> Any:\n",
    "    attribute = default if key not in dict_to_check.keys() else dict_to_check[key]\n",
    "    return attribute\n",
    "\n",
    "def _get_initial_params(proposal, init_strategy: str, num_chains: int, num_workers: int, show_progress_bars: bool, **kwargs,) -> Tensor: \n",
    "    # Build init function\n",
    "    init_fn = proposal._build_mcmc_init_fn(proposal.proposal, proposal.potential_fn, transform=proposal.theta_transform, init_strategy=init_strategy, **kwargs,)\n",
    "\n",
    "    # Parallelize inits for resampling only.\n",
    "    if num_workers > 1 and (init_strategy == \"resample\" or init_strategy == \"sir\"):\n",
    "        def seeded_init_fn(seed):\n",
    "            torch.manual_seed(seed)\n",
    "            return init_fn()\n",
    "\n",
    "        seeds = torch.randint(high=10_000_000, size=(num_chains,))\n",
    "\n",
    "        # Generate initial params parallelized over num_workers.\n",
    "        with tqdm_joblib(tqdm(range(num_chains), disable=not show_progress_bars, desc=f\"\"\"Generating {num_chains} MCMC inits with {num_workers} workers.\"\"\", total=num_chains,)):\n",
    "            initial_params = torch.cat(Parallel(n_jobs=num_workers)(delayed(seeded_init_fn)(seed) for seed in seeds))\n",
    "    else:\n",
    "        initial_params = torch.cat([init_fn() for _ in range(num_chains)])\n",
    "    return initial_params\n",
    "    \n",
    "def _slice_np_mcmc(proposal, num_samples: int, potential_function: Callable, initial_params: Tensor, thin: int, warmup_steps: int, vectorized: bool = False, num_workers: int = 1, init_width: Union[float, ndarray] = 0.01, show_progress_bars: bool = True,) -> Tensor:\n",
    "    num_chains, dim_samples = initial_params.shape\n",
    "        \n",
    "    if not vectorized:\n",
    "        SliceSamplerMultiChain = SliceSamplerSerial\n",
    "    else:\n",
    "        SliceSamplerMultiChain = SliceSamplerVectorized\n",
    "\n",
    "    posterior_sampler = SliceSamplerMultiChain(init_params=tensor2numpy(initial_params), log_prob_fn=potential_function, num_chains=num_chains, thin=thin, verbose=show_progress_bars, num_workers=num_workers, init_width=init_width,)\n",
    "    warmup_ = warmup_steps * thin\n",
    "    num_samples_ = ceil((num_samples * thin) / num_chains)\n",
    "    # Run mcmc including warmup\n",
    "    samples = run(posterior_sampler, log_prob_fn=potential_function, num_samples = (warmup_ + num_samples_), init_params = tensor2numpy(initial_params))\n",
    "    samples = samples[:, warmup_steps:, :]  # discard warmup steps\n",
    "    samples = torch.from_numpy(samples)  # chains x samples x dim\n",
    "\n",
    "    # Save posterior sampler.\n",
    "    proposal._posterior_sampler = posterior_sampler\n",
    "\n",
    "    # Save sample as potential next init (if init_strategy == 'latest_sample').\n",
    "    proposal._mcmc_init_params = samples[:, -1, :].reshape(num_chains, dim_samples)\n",
    "\n",
    "    # Collect samples from all chains.\n",
    "    samples = samples.reshape(-1, dim_samples)[:num_samples, :]\n",
    "    assert samples.shape[0] == num_samples\n",
    "    return samples.type(torch.float32).to(proposal._device)\n",
    "\n",
    "def sample_my_fun(proposal, sample_shape: Shape = torch.Size(), x: Optional[Tensor] = None, method: Optional[str] = None, thin: Optional[int] = None, warmup_steps: Optional[int] = None, num_chains: Optional[int] = None, init_strategy: Optional[str] = None, init_strategy_parameters: Optional[Dict[str, Any]] = None,\n",
    "                   init_strategy_num_candidates: Optional[int] = None, mcmc_parameters: Dict = {}, mcmc_method: Optional[str] = None, sample_with: Optional[str] = None, num_workers: Optional[int] = None, show_progress_bars: bool = True,) -> Tensor:\n",
    "    \n",
    "    proposal.potential_fn.set_x(proposal._x_else_default_x(x))\n",
    "\n",
    "    # Replace arguments that were not passed with their default.\n",
    "    method = proposal.method if method is None else method\n",
    "    thin = proposal.thin if thin is None else thin\n",
    "    warmup_steps = proposal.warmup_steps if warmup_steps is None else warmup_steps\n",
    "    num_chains = proposal.num_chains if num_chains is None else num_chains\n",
    "    init_strategy = proposal.init_strategy if init_strategy is None else init_strategy\n",
    "    num_workers = proposal.num_workers if num_workers is None else num_workers\n",
    "    init_strategy_parameters = (proposal.init_strategy_parameters if init_strategy_parameters is None else init_strategy_parameters)\n",
    "\n",
    "    if init_strategy_num_candidates is not None:\n",
    "        warn(\"\"\"Passing `init_strategy_num_candidates` is deprecated as of sbi v0.19.0. Instead, use e.g.,`init_strategy_parameters={\"num_candidate_samples\": 1000}`\"\"\")\n",
    "        proposal.init_strategy_parameters[\"num_candidate_samples\"] = (init_strategy_num_candidates)\n",
    "    if sample_with is not None:\n",
    "        raise ValueError(f\"You set `sample_with={sample_with}`. As of sbi v0.18.0, setting `sample_with` is no longer supported. You have to rerun `.build_posterior(sample_with={sample_with}).`\")\n",
    "    if mcmc_method is not None:\n",
    "        warn(\"You passed `mcmc_method` to `.sample()`. As of sbi v0.18.0, this is deprecated and will be removed in a future release. Use `method` instead of `mcmc_method`.\")\n",
    "        method = mcmc_method\n",
    "    if mcmc_parameters:\n",
    "        warn(\"You passed `mcmc_parameters` to `.sample()`. As of sbi v0.18.0, this is deprecated and will be removed in a future release. Instead, pass the variable to `.sample()` directly, e.g. `posterior.sample((1,), num_chains=5)`.\")\n",
    "    # The following lines are only for backwards compatibility with sbi v0.17.2 or older.\n",
    "    m_p = mcmc_parameters  # define to shorten the variable name\n",
    "    method = _maybe_use_dict_entry(method, \"mcmc_method\", m_p)\n",
    "    thin = _maybe_use_dict_entry(thin, \"thin\", m_p)\n",
    "    warmup_steps = _maybe_use_dict_entry(warmup_steps, \"warmup_steps\", m_p)\n",
    "    num_chains = _maybe_use_dict_entry(num_chains, \"num_chains\", m_p)\n",
    "    init_strategy = _maybe_use_dict_entry(init_strategy, \"init_strategy\", m_p)\n",
    "    proposal.potential_ = proposal._prepare_potential(method)  # type: ignore\n",
    "\n",
    "    initial_params = _get_initial_params(proposal, init_strategy, num_chains, num_workers, show_progress_bars, **init_strategy_parameters,)\n",
    "    num_samples = torch.Size(sample_shape).numel()\n",
    "\n",
    "    track_gradients = method in (\"hmc\", \"nuts\")\n",
    "    with torch.set_grad_enabled(track_gradients):\n",
    "        if method in (\"slice_np\", \"slice_np_vectorized\"):\n",
    "            transformed_samples = _slice_np_mcmc(proposal, num_samples=num_samples, potential_function=proposal.potential_, initial_params=initial_params, thin=thin, warmup_steps=warmup_steps, vectorized=(method == \"slice_np_vectorized\"), num_workers=num_workers, show_progress_bars=show_progress_bars,)\n",
    "        elif method in (\"hmc\", \"nuts\", \"slice\"):\n",
    "            transformed_samples = _pyro_mcmc(proposal, num_samples=num_samples, potential_function=proposal.potential_, initial_params=initial_params, mcmc_method=method, thin=thin, warmup_steps=warmup_steps, num_chains=num_chains, show_progress_bars=show_progress_bars,)\n",
    "        else:\n",
    "            raise NameError\n",
    "\n",
    "    samples = proposal.theta_transform.inv(transformed_samples)\n",
    "\n",
    "    return samples.reshape((*sample_shape, -1))  # type: ignore\n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "def simulator_seeded(simulator: Callable, theta: Tensor, seed: int) -> Tensor:\n",
    "    import torch\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    with torch.random.fork_rng(devices=[]):\n",
    "        torch.manual_seed(seed)\n",
    "        return simulator(theta)\n",
    "\n",
    "def simulate_in_batches(simulator: Callable, theta: Tensor, sim_batch_size: int = 1, num_workers: int = 1 , seed: Optional[int] = None, show_progress_bars: bool = True, ) -> Tensor:\n",
    "    num_sims, *_ = theta.shape\n",
    "    seed_all_backends(seed)\n",
    "    if num_sims == 0:\n",
    "        x = torch.tensor([])\n",
    "    elif sim_batch_size is not None and sim_batch_size < num_sims:\n",
    "        batches = split(theta, sim_batch_size, dim=0)\n",
    "        \n",
    "        if num_workers != 1:\n",
    "            batch_seeds = randint(high=1_000_000, size=(len(batches),))\n",
    "            with tqdm_joblib(tqdm(batches, disable=not show_progress_bars, total = len(batches), desc=f\"Running {num_sims} simulations in {len(batches)} batches ({num_workers} cores)\",)) as _:\n",
    "                simulation_outputs = Parallel(n_jobs=num_workers)(delayed(simulator_seeded)(simulator, batch, batch_seed) for batch, batch_seed in zip(batches, batch_seeds))\n",
    "        else:\n",
    "            pbar = tqdm(total=num_sims, disable=not show_progress_bars, desc=f\"Running {num_sims} simulations.\", )\n",
    "            with pbar:\n",
    "                simulation_outputs = []\n",
    "                for batch in batches:\n",
    "                    simulation_outputs.append(simulator_seeded(simulator, batch, seed))\n",
    "                    pbar.update(sim_batch_size)\n",
    "        x = cat(simulation_outputs, dim=0)\n",
    "    else:\n",
    "        x = simulator(theta)\n",
    "    return x\n",
    "\n",
    "def simulate_for_sbi(round_idx: int, simulator: Callable, proposal: Any, num_simulations: int, num_workers: int = 1, simulation_batch_size: int = 1, seed: Optional[int] = None, show_progress_bar: bool = True)-> Tuple[Tensor, Tensor]:\n",
    "    if round_idx == 0:\n",
    "        theta = proposal.sample((num_simulations,))\n",
    "    else:\n",
    "        theta = sample_my_fun(proposal, (num_simulations,), num_workers = num_workers, num_chains = 4) # because only in first round proposal is boxuniform, then it is mcmcposterior object\n",
    "    \n",
    "    x = simulate_in_batches(simulator=simulator, theta=theta, sim_batch_size=simulation_batch_size, num_workers=num_workers, seed=seed, show_progress_bars=show_progress_bar)\n",
    "    \n",
    "    return theta, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "625fb39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 64 simulations in 8 batches (8 cores): 100%|██████████| 8/8 [00:32<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 220 epochs.\n",
      "\n",
      "Round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 4 MCMC inits with 8 workers.: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n",
      "/project/home23/av420/.local/lib/python3.11/site-packages/sbi/samplers/mcmc/slice_numpy.py:393: UserWarning: Parallelization of vectorized slice sampling not implement, running\n",
      "                serially.\n",
      "  warn(\n",
      "Tuning bracket width...: 100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "Generating samples:  73%|███████▎  | 189/260 [00:26<00:10,  7.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rounds):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m     theta, x \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_for_sbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msimulation_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     density_estimator \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mappend_simulations(theta, x)\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     93\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mbuild_posterior(density_estimator)\n",
      "Cell \u001b[0;32mIn[21], line 318\u001b[0m, in \u001b[0;36msimulate_for_sbi\u001b[0;34m(round_idx, simulator, proposal, num_simulations, num_workers, simulation_batch_size, seed, show_progress_bar)\u001b[0m\n\u001b[1;32m    316\u001b[0m     theta \u001b[38;5;241m=\u001b[39m proposal\u001b[38;5;241m.\u001b[39msample((num_simulations,))\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     theta \u001b[38;5;241m=\u001b[39m \u001b[43msample_my_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mslice_np_vectorized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# because only in first round proposal is boxuniform, then it is mcmcposterior object\u001b[39;00m\n\u001b[1;32m    320\u001b[0m x \u001b[38;5;241m=\u001b[39m simulate_in_batches(simulator\u001b[38;5;241m=\u001b[39msimulator, theta\u001b[38;5;241m=\u001b[39mtheta, sim_batch_size\u001b[38;5;241m=\u001b[39msimulation_batch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, seed\u001b[38;5;241m=\u001b[39mseed, show_progress_bars\u001b[38;5;241m=\u001b[39mshow_progress_bar)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m theta, x\n",
      "Cell \u001b[0;32mIn[21], line 270\u001b[0m, in \u001b[0;36msample_my_fun\u001b[0;34m(proposal, sample_shape, x, method, thin, warmup_steps, num_chains, init_strategy, init_strategy_parameters, init_strategy_num_candidates, mcmc_parameters, mcmc_method, sample_with, num_workers, show_progress_bars)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(track_gradients):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_np\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_np_vectorized\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m         transformed_samples \u001b[38;5;241m=\u001b[39m \u001b[43m_slice_np_mcmc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpotential_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproposal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpotential_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_np_vectorized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhmc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m         transformed_samples \u001b[38;5;241m=\u001b[39m _pyro_mcmc(proposal, num_samples\u001b[38;5;241m=\u001b[39mnum_samples, potential_function\u001b[38;5;241m=\u001b[39mproposal\u001b[38;5;241m.\u001b[39mpotential_, initial_params\u001b[38;5;241m=\u001b[39minitial_params, mcmc_method\u001b[38;5;241m=\u001b[39mmethod, thin\u001b[38;5;241m=\u001b[39mthin, warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps, num_chains\u001b[38;5;241m=\u001b[39mnum_chains, show_progress_bars\u001b[38;5;241m=\u001b[39mshow_progress_bars,)\n",
      "Cell \u001b[0;32mIn[21], line 216\u001b[0m, in \u001b[0;36m_slice_np_mcmc\u001b[0;34m(proposal, num_samples, potential_function, initial_params, thin, warmup_steps, vectorized, num_workers, init_width, show_progress_bars)\u001b[0m\n\u001b[1;32m    214\u001b[0m num_samples_ \u001b[38;5;241m=\u001b[39m ceil((num_samples \u001b[38;5;241m*\u001b[39m thin) \u001b[38;5;241m/\u001b[39m num_chains)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Run mcmc including warmup\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposterior_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpotential_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarmup_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_samples_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor2numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples[:, warmup_steps:, :]  \u001b[38;5;66;03m# discard warmup steps\u001b[39;00m\n\u001b[1;32m    218\u001b[0m samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(samples)  \u001b[38;5;66;03m# chains x samples x dim\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 139\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(SliceSamplerSerial, log_prob_fn, num_samples, init_params, num_chains, thin, verbose, num_workers)\u001b[0m\n\u001b[1;32m    137\u001b[0m     seed_all_backends(seed)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm_joblib(tqdm(\u001b[38;5;28mrange\u001b[39m(num_chains), disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose \u001b[38;5;129;01mor\u001b[39;00m num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_chains\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MCMC chains with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_workers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m worker\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mnum_workers\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mnum_chains,)):\n\u001b[0;32m--> 139\u001b[0m     all_samples \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_fun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSliceSamplerSerial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_params_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitial_params_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(all_samples)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    141\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mreshape(num_chains, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim_samples)  \u001b[38;5;66;03m# chains, samples, dim\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "Cell \u001b[0;32mIn[21], line 112\u001b[0m, in \u001b[0;36mrun_fun\u001b[0;34m(SliceSamplerSerial, num_samples, inits, seed, log_prob_fn, thin, tuning, verbose, init_width, max_width, num_workers, rng, show_info, logger)\u001b[0m\n\u001b[1;32m    109\u001b[0m     rng\u001b[38;5;241m.\u001b[39mshuffle(order)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m order:\n\u001b[0;32m--> 112\u001b[0m         posterior_sampler\u001b[38;5;241m.\u001b[39mx[i], _ \u001b[38;5;241m=\u001b[39m \u001b[43mposterior_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_from_conditional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m samples[n] \u001b[38;5;241m=\u001b[39m posterior_sampler\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    116\u001b[0m posterior_sampler\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m=\u001b[39m posterior_sampler\u001b[38;5;241m.\u001b[39mlp_f(posterior_sampler\u001b[38;5;241m.\u001b[39mx)\n",
      "Cell \u001b[0;32mIn[21], line 68\u001b[0m, in \u001b[0;36mSliceSampler._sample_from_conditional\u001b[0;34m(self, i, cxi, rng)\u001b[0m\n\u001b[1;32m     65\u001b[0m ux \u001b[38;5;241m=\u001b[39m lx \u001b[38;5;241m+\u001b[39m wi\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# find lower bracket end\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mLi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m logu \u001b[38;5;129;01mand\u001b[39;00m cxi \u001b[38;5;241m-\u001b[39m lx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width:\n\u001b[1;32m     69\u001b[0m     lx \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m wi\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# find upper bracket end\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 57\u001b[0m, in \u001b[0;36mSliceSampler._sample_from_conditional.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain not initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# conditional log prob\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m Li \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlp_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m wi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth[i]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# sample a slice uniformly\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sbi/utils/potentialutils.py:41\u001b[0m, in \u001b[0;36mtransformed_potential\u001b[0;34m(theta, potential_fn, theta_transform, device, track_gradients)\u001b[0m\n\u001b[1;32m     38\u001b[0m theta \u001b[38;5;241m=\u001b[39m theta_transform\u001b[38;5;241m.\u001b[39minv(transformed_theta)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     39\u001b[0m log_abs_det \u001b[38;5;241m=\u001b[39m theta_transform\u001b[38;5;241m.\u001b[39mlog_abs_det_jacobian(theta, transformed_theta)\n\u001b[0;32m---> 41\u001b[0m posterior_potential \u001b[38;5;241m=\u001b[39m \u001b[43mpotential_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m posterior_potential_transformed \u001b[38;5;241m=\u001b[39m posterior_potential \u001b[38;5;241m-\u001b[39m log_abs_det\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m posterior_potential_transformed\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sbi/inference/potentials/likelihood_based_potential.py:92\u001b[0m, in \u001b[0;36mLikelihoodBasedPotential.__call__\u001b[0;34m(self, theta, track_gradients)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the potential $\\log(p(x_o|\\theta)p(\\theta))$.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    The potential $\\log(p(x_o|\\theta)p(\\theta))$.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Calculate likelihood over trials and in one batch.\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m log_likelihood_trial_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_log_likelihoods_over_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_o\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlikelihood_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrack_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_likelihood_trial_sum \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39mlog_prob(theta)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sbi/inference/potentials/likelihood_based_potential.py:140\u001b[0m, in \u001b[0;36m_log_likelihoods_over_trials\u001b[0;34m(x, theta, net, track_gradients)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Calculate likelihood in one batch.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(track_gradients):\n\u001b[0;32m--> 140\u001b[0m     log_likelihood_trial_batch \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_repeated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_repeated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Reshape to (x-trials x parameters), sum over trial-log likelihoods.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     log_likelihood_trial_sum \u001b[38;5;241m=\u001b[39m log_likelihood_trial_batch\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    143\u001b[0m         x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/distributions/base.py:40\u001b[0m, in \u001b[0;36mDistribution.log_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of input items must be equal to number of context items.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         )\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/flows/base.py:39\u001b[0m, in \u001b[0;36mFlow._log_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context):\n\u001b[1;32m     38\u001b[0m     embedded_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_net(context)\n\u001b[0;32m---> 39\u001b[0m     noise, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedded_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution\u001b[38;5;241m.\u001b[39mlog_prob(noise, context\u001b[38;5;241m=\u001b[39membedded_context)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_prob \u001b[38;5;241m+\u001b[39m logabsdet\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/transforms/base.py:56\u001b[0m, in \u001b[0;36mCompositeTransform.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     funcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transforms\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cascade\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/transforms/base.py:50\u001b[0m, in \u001b[0;36mCompositeTransform._cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m total_logabsdet \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnew_zeros(batch_size)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m funcs:\n\u001b[0;32m---> 50\u001b[0m     outputs, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     total_logabsdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logabsdet\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, total_logabsdet\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/transforms/autoregressive.py:38\u001b[0m, in \u001b[0;36mAutoregressiveTransform.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m     autoregressive_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoregressive_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     outputs, logabsdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elementwise_forward(inputs, autoregressive_params)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, logabsdet\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/transforms/made.py:282\u001b[0m, in \u001b[0;36mMADE.forward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    281\u001b[0m     temps \u001b[38;5;241m=\u001b[39m block(temps, context)\n\u001b[0;32m--> 282\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/nflows/transforms/made.py:72\u001b[0m, in \u001b[0;36mMaskedLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "headers = [\"k1\", \"k2\",  \"k3\"]          # parameters to be inferred\n",
    "num_timesteps = 100\n",
    "\n",
    "# FOR SNLE\n",
    "prior_min = 0.01                        # same for all parameters\n",
    "prior_max = 250                        # same for all parameters\n",
    "num_rounds = 3                        # how many rounds of SNLE\n",
    "num_simulations = 64               # in each round\n",
    "\n",
    "# To simulate in batches, simulation_batch_size must not be None and simulation_batch_size < num_simulations\n",
    "# To parallelise, set number of CPUs to be used.\n",
    "simulation_batch_size = 8\n",
    " # run os.cpu_count() to see number of available CPUs\n",
    "CPUs_to_use = 8\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "# # FOR MCMC\n",
    "# num_iterations = 10_000 # total in all chains\n",
    "# interval_to_calculate_acceptance_rate = 100\n",
    "# burn_in_fraction = 0.3\n",
    "# num_chains = 4\n",
    "\n",
    "####################################################################################################################################\n",
    "\n",
    "num_workers = CPUs_to_use\n",
    "if CPUs_to_use > os.cpu_count():\n",
    "    raise ValueError(f\"Cannot use more CPUs than are available. Available CPUs: {os.cpu_count()}\")\n",
    "\n",
    "t = np.linspace(0, 100, num_timesteps)\n",
    "\n",
    "param_dict = {'k1': 246.96291990024542, 'k2': 246.96291990024542, 'k3': 246.96291990024542, 'n1': 5, 'n2': 5, 'n3': 5, 'dm1': 1.143402097500176, 'dm2': 1.143402097500176, 'dm3': 1.143402097500176,\n",
    "              'dp1': 0.7833664565550977, 'dp2': 0.7833664565550977, 'dp3': 0.7833664565550977, 'a1': 24.78485282457379, 'a2': 24.78485282457379, 'a3': 24.78485282457379,\n",
    "              'g1': 0.024884149937163258, 'g2': 0.024884149937163258, 'g3': 0.024884149937163258, 'b1': 33.82307682700831, 'b2': 33.82307682700831, 'b3': 33.82307682700831}\n",
    "\n",
    "all_params = 'a1', 'a2', 'a3', 'g1', 'g2', 'g3', 'dm1', 'dm2', 'dm3', 'dp1', 'dp2', 'dp3', 'b1', 'b2', 'b3', 'n1', 'n2', 'n3', 'k1', 'k2', 'k3'\n",
    "new_param_dictx ={}\n",
    "for param in all_params:\n",
    "    if param not in headers:\n",
    "        new_param_dictx[param] = param_dict[param]\n",
    "    elif param in headers:\n",
    "        new_param_dictx[param] = param\n",
    "\n",
    "def my_simulator(theta):\n",
    "    def model(variables, t, new_param_dict):\n",
    "        m1, p1, m2, p2, m3, p3 = variables\n",
    "        \n",
    "        dm1dt = -new_param_dict['dm1']*m1 + (new_param_dict['a1'] / (1 + (p2/new_param_dict['k1'])**new_param_dict['n1'])) + new_param_dict['g1']\n",
    "        dp1dt = (new_param_dict['b1']*m1) - (new_param_dict['dp1']*p1)\n",
    "        dm2dt = -new_param_dict['dm2']*m2 + (new_param_dict['a2'] / (1 + (p3/new_param_dict['k2'])**new_param_dict['n2'])) + new_param_dict['g2']\n",
    "        dp2dt = (new_param_dict['b2']*m2) - (new_param_dict['dp2']*p2)\n",
    "        dm3dt = -new_param_dict['dm3']*m3 + (new_param_dict['a3'] / (1 + (p1/new_param_dict['k3'])**new_param_dict['n3'])) + new_param_dict['g3']\n",
    "        dp3dt = (new_param_dict['b3']*m3) - (new_param_dict['dp3']*p3)\n",
    "\n",
    "        return [dm1dt, dp1dt, dm2dt, dp2dt, dm3dt, dp3dt]\n",
    "\n",
    "    def solve_ode(theta, t, new_param_dict = new_param_dictx):\n",
    "        for i in range(len(headers)):\n",
    "            new_param_dict[headers[i]] = theta[i]\n",
    "\n",
    "        initial_conditions = np.array([0, 2, 0, 1, 0, 3], dtype=np.float32)\n",
    "        solution = odeint(model, initial_conditions, t, args=(new_param_dict,))\n",
    "        return torch.tensor(solution, dtype=torch.float32).flatten() # Flatten tensor to size [600]\n",
    "    return solve_ode(theta, t)\n",
    "\n",
    "def unflatten(solution):\n",
    "    variables = ['m1', 'p1', 'm2', 'p2', 'm3', 'p3']\n",
    "    solution_unflattened = torch.zeros((num_timesteps, len(variables)))\n",
    "    for timestep in range(num_timesteps):\n",
    "        for variable_idx, variable in enumerate(variables):\n",
    "            solution_unflattened[timestep, variable_idx]=solution[timestep+variable_idx]\n",
    "    return solution_unflattened\n",
    "\n",
    "true_params = tuple(param_dict[parameter] for parameter in headers)\n",
    "true_solutions = my_simulator(true_params)\n",
    "true_solutions_unflattened = unflatten(true_solutions)\n",
    "#####################################################################################################################\n",
    "\n",
    "num_dim = len(true_params)\n",
    "prior = utils.BoxUniform(low=prior_min * torch.ones(num_dim), high=prior_max * torch.ones(num_dim))\n",
    "simulator, prior = prepare_for_sbi(my_simulator, prior)\n",
    "\n",
    "my_density_estimator = likelihood_nn(model=\"maf\", hidden_features=50, num_transforms=3)\n",
    "\n",
    "inference = SNLE(prior = prior, density_estimator = my_density_estimator) # Initialise inference\n",
    "posteriors = [] # Empty list to contain posterior after each round\n",
    "proposal = prior # For the first round proposal = prior, then updated (sequentiality)\n",
    "\n",
    "for _ in range(num_rounds):\n",
    "    print(f\"Round {_+1}\")\n",
    "    theta, x = simulate_for_sbi(_, simulator, proposal, num_simulations = num_simulations, simulation_batch_size = simulation_batch_size, num_workers = num_workers)\n",
    "    density_estimator = inference.append_simulations(theta, x).train()\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    posteriors.append(posterior)\n",
    "    proposal = posterior.set_default_x(true_solutions)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(posterior)\n",
    "posterior_samples = sample_my_fun(posterior, (24,), num_chains = 1) # sample to plot the posteriors\n",
    "\n",
    "snle_data = pd.DataFrame(data=posterior_samples, columns=headers)\n",
    "snle_data.to_csv(f'{num_dim}p-{num_rounds}*{num_simulations}.csv')\n",
    "\n",
    "# Calculate quantiles of posterior samples\n",
    "posterior_quantiles = np.percentile(posterior_samples, [1, 99], axis=0)\n",
    "\n",
    "# Define custom limits slightly larger than the range of the central 98% of the posterior samples\n",
    "custom_limits = [(posterior_quantiles[0][i] - 0.2 * (posterior_quantiles[1][i] - posterior_quantiles[0][i]), posterior_quantiles[1][i] + 0.2 * (posterior_quantiles[1][i] - posterior_quantiles[0][i])) for i in range(num_dim)]\n",
    "\n",
    "# Plot pair plots with custom limits\n",
    "_ = analysis.pairplot(posterior_samples, limits=custom_limits, figsize=(8, 8), labels=headers)\n",
    "\n",
    "plt.savefig(f'{num_dim}p-{num_rounds}*{num_simulations}-P_customlimits.png')\n",
    "\n",
    "_ = analysis.pairplot(posterior_samples, limits=[[prior_min, prior_max]]*num_dim, figsize=(8, 8), labels=headers)\n",
    "\n",
    "plt.savefig(f'{num_dim}p-{num_rounds}*{num_simulations}-P_priorlimits.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6d4b2",
   "metadata": {},
   "source": [
    "From ChatGPT:\n",
    "1. The neural network (self._neural_net) is set to training mode (self._neural_net.train()). This ensures that layers like dropout and batch normalization behave differently during training compared to evaluation.\n",
    "\n",
    "2. The loop iterates over the training data in batches (for batch in train_loader). Each batch typically consists of input samples (x) and their corresponding parameters (theta).\n",
    "\n",
    "3. The gradients of the model parameters are reset to zero (self.optimizer.zero_grad()) before computing the gradients for the current batch. This prevents gradient accumulation across batches.\n",
    "\n",
    "4. The model is then applied to the input samples to compute the losses (train_losses) using the loss function (self._loss). This loss function typically compares the model's predictions for x given theta with the ground truth. The loss function returns the -log probability i.e. log likelihood of samples\n",
    "                \n",
    "    i. Separates the input data x into continuous and discrete components using a helper function _separate_x(x)\n",
    "    \n",
    "    ii. The log probability for the each part is computed using a neural network with theta  as context (continuous part may be transformed into log-space if needed)\n",
    "    \n",
    "    iii. The log probabilities for the discrete and continuous parts are combined into a joint log probability (log_probs_combined)\n",
    "    \n",
    "    iv. If the continuous data is transformed to log-space, the log absolute determinant Jacobian of the transformation is subtracted from the joint log probability\n",
    "    \n",
    "    v. Returns the joint log probability of p(x∣θ) for each sample in the batch\n",
    "\n",
    "5. The losses are averaged across the batch to obtain a single scalar value representing the overall loss for the batch (train_loss).\n",
    "\n",
    "6. The gradients of the loss with respect to the model parameters are computed using backpropagation (train_loss.backward()).\n",
    "\n",
    "7. Optionally, the gradients are clipped to prevent them from growing too large and causing instability during training (clip_grad_norm_()).\n",
    "\n",
    "8. An optimizer (self.optimizer) updates the model parameters based on the computed gradients (self.optimizer.step())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables = ['m1', 'p1', 'm2', 'p2', 'm3', 'p3']\n",
    "# # Define a function to simulate a single sample\n",
    "# def simulate_sample(sample, seed):\n",
    "#     np.random.seed(seed)\n",
    "#     return unflatten(my_simulator(sample))\n",
    "\n",
    "# # Use joblib to parallelize the simulation\n",
    "# batches = posterior_samples.split(num_workers)\n",
    "# seeds = randint(high=1_000_000, size=(len(batches),))\n",
    "# with tqdm_joblib(tqdm(batches, total = len(batches), desc=f\"Running {len(posterior_samples2)} simulations in {len(batches)} batches ({num_workers} cores)\",)) as _: \n",
    "#     results = Parallel(n_jobs=num_workers)(delayed(simulate_sample)(sample, seed) for sample, seed in zip(posterior_samples2, seeds))\n",
    "\n",
    "# # Now process the results to calculate quartiles as before\n",
    "# variable_lists = [[[] for _ in range(num_timesteps)] for _ in range(len(variables))]\n",
    "# for data in results:\n",
    "#     for timepoint in range(num_timesteps):\n",
    "#         for i in range(len(variables)):\n",
    "#             variable_lists[i][timepoint].append(data[timepoint][i])\n",
    "\n",
    "# quantile_lists = [[] for _ in range(len(variables))]\n",
    "# for variable_list in range(len(variables)):\n",
    "#     for timepoint in variable_lists[variable_list]:\n",
    "#         timepoint_array = np.array(timepoint)  # Convert to numpy array\n",
    "#         q1 = np.percentile(timepoint_array, 25, axis=0)\n",
    "#         q3 = np.percentile(timepoint_array, 75, axis=0)\n",
    "#         quantile_lists[variable_list].append([q1, q3])\n",
    "\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "# for idx, ax in enumerate(axes.flat):\n",
    "#     true_data = true_solutions_unflattened[:, idx]\n",
    "#     ax.plot(t, true_data, label='true', color='blue')\n",
    "\n",
    "#     quantile_array = np.array(quantile_lists[idx])\n",
    "#     ax.plot(t, quantile_array[:, 0], label='q1', color='orange', linestyle='--')\n",
    "#     ax.plot(t, quantile_array[:, 1], label='q3', color='green', linestyle='--')\n",
    "\n",
    "#     ax.set_xlabel('t')\n",
    "#     ax.set_ylabel(variables[idx])\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.savefig(f'{num_dim}p-{num_rounds}*{num_simulations}-T.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "489ceeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 24 simulations in 8 batches (8 cores): 100%|██████████| 8/8 [00:03<00:00,  2.14it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of bounds for axis 0 with size 24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, batch_results \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(indexes, results):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_results):\n\u001b[0;32m---> 22\u001b[0m         \u001b[43mraw_trajectories\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_trajectories)\n\u001b[1;32m     26\u001b[0m tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(raw_trajectories, [\u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;241m97.5\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 24 is out of bounds for axis 0 with size 24"
     ]
    }
   ],
   "source": [
    "variables = ['m1', 'p1', 'm2', 'p2', 'm3', 'p3']\n",
    "\n",
    "raw_trajectories=np.zeros([len(posterior_samples), num_timesteps, len(variables)])\n",
    "\n",
    "def simulate_sample(batch, seed):\n",
    "    np.random.seed(seed)\n",
    "    result = []\n",
    "    for i in range(len(batch)):\n",
    "        result.append(my_simulator(batch[i]).reshape(num_timesteps, len(variables)))\n",
    "        return result\n",
    "\n",
    "# Use joblib to parallelize the simulation\n",
    "batches = split(posterior_samples, int(len(posterior_samples)//num_workers), dim =0)\n",
    "indexes = np.arange(len(posterior_samples))\n",
    "\n",
    "seeds = randint(high=1_000_000, size=(len(batches),))\n",
    "with tqdm_joblib(tqdm(batches, total = len(batches), desc=f\"Running {len(posterior_samples)} simulations in {len(batches)} batches ({num_workers} cores)\",)) as _: \n",
    "    results = Parallel(n_jobs=num_workers)(delayed(simulate_sample)(batch, seed) for batch, seed in zip(batches, seeds))\n",
    "\n",
    "for index, batch_results in zip(indexes, results):\n",
    "    for i, result in enumerate(batch_results):\n",
    "        raw_trajectories[index * num_workers + i] = result\n",
    "\n",
    "print(raw_trajectories)\n",
    "\n",
    "tr = np.percentile(raw_trajectories, [2.5, 97.5], axis=0)\n",
    "\n",
    "fig, ax=plt.subplots(3,2,figsize=(15,9))\n",
    "ax = ax.ravel()\n",
    "col=[\"blue\",\"blue\"]\n",
    "variables = [\"m1\",\"p1\",\"m2\",\"p2\", \"m3\", \"p3\"]\n",
    "for i in range(6):\n",
    "    for j in range(2):\n",
    "        ax[i].plot(tr[j,:,i],alpha=0.4,linestyle='dotted',linewidth=1, color='black')\n",
    "    ax[i].plot(true_solutions.reshape(100, 6)[:,i],linewidth=0.6,color='black', label = 'true')\n",
    "    ax[i].fill_between(t, tr[0, :, i],tr[1, :, i], alpha=0.4, color='skyblue')\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba300879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getDif(indexes, arrayData):\t\n",
    "#     arrLen = len(indexes)\n",
    "#     sum = 0\n",
    "#     for i, ind in enumerate(indexes):\n",
    "#         if i == arrLen - 1:\n",
    "#             break\n",
    "#         sum += arrayData[ind] - arrayData[indexes[i + 1]]\n",
    "        \n",
    "#     #add last peak - same as substracting it from zero \n",
    "#     sum += arrayData[indexes[-1:]]  \n",
    "#     return sum   \n",
    "    \n",
    "# def getSTD(indexes, arrayData, window):\n",
    "#     numPeaks = len(indexes)\n",
    "#     arrLen = len(arrayData)\n",
    "#     sum = 0\n",
    "#     for ind in indexes:\n",
    "#         minInd = max(0, ind - window)\n",
    "#         maxInd = min(arrLen, ind + window)\n",
    "#         sum += np.std(arrayData[minInd:maxInd])  \n",
    "        \n",
    "#     sum = sum/numPeaks \t#The 1/P factor\n",
    "#     return sum\n",
    "    \n",
    "# def getFrequencies(y):\n",
    "#     res = abs(fft.rfft(y))  #Real FT\n",
    "#     #normalize the amplitudes \n",
    "#     #res = res/math.ceil(1/2) #Normalise with a factor of 1/2\n",
    "#     return res\n",
    "\n",
    "# def costTwo(Y, getAmplitude = False): #Yes\n",
    "#     p1 = Y[:,1]  #Get the first column\n",
    "#     fftData = getFrequencies(p1)    #Get frequencies of FFT of the first column  \n",
    "#     fftData = np.array(fftData) \n",
    "#     #find peaks using very low threshold and minimum distance\n",
    "#     indexes = peakutils.indexes(fftData, thres=0.02/max(fftData), min_dist=1)  #Just find peaks\n",
    "#     #in case of no oscillations return 0 \n",
    "#     if len(indexes) == 0:     \n",
    "#         return 0\n",
    "#     #if amplitude is greater than 400nM\n",
    "#     #global amp\n",
    "#     #amp = np.max(fftData[indexes])\n",
    "#     #if amp > 400: #If bigger than 400, then cost is 0, not viable\n",
    "#       #  return 0, \n",
    "#     fitSamples = fftData[indexes]\n",
    "#     std = getSTD(indexes, fftData, 1)  #get sd of peaks at a window of 1 (previous peak)\n",
    "#     diff = getDif(indexes, fftData)  #Get differences in peaks\n",
    "#     cost = std + diff #Sum them\n",
    "#     #print(cost)   \n",
    "#     if getAmplitude:\n",
    "#         return cost, amp\n",
    "#     return int(cost)\n",
    "\n",
    "# def euclidean_distance_multiple_trajectories(truth, simulations):\n",
    "#     num_trajectories = len(simulations)\n",
    "#     total_distance = 0.0\n",
    "\n",
    "#     for i in range(num_trajectories):\n",
    "#         observed_data = truth[i]\n",
    "#         simulated_data = simulations[i]\n",
    "\n",
    "#         # Calculate the Euclidean distance between observed and simulated data\n",
    "#         euclidean_distance = np.linalg.norm(observed_data - simulated_data)\n",
    "\n",
    "#         # Accumulate the distances\n",
    "#         total_distance += euclidean_distance\n",
    "\n",
    "#     # Average the distances over all trajectories\n",
    "#     average_distance = total_distance / num_trajectories\n",
    "\n",
    "#     return average_distance\n",
    "\n",
    "# def get_distance(truth, simulation):\n",
    "#     timepoints = int(len(truth))\n",
    "#     third = int(timepoints / 3)\n",
    "#     observed = truth[third:timepoints]\n",
    "#     simulated = simulation[third:timepoints] # Discard the first third\n",
    "#     euclidean_distance = euclidean_distance_multiple_trajectories(observed, simulated)\n",
    "#     penalising_factor = np.abs(np.abs(costTwo(simulation)) - 200)\n",
    "#     if costTwo(simulation) >= 200:\n",
    "#         return euclidean_distance\n",
    "#     else:\n",
    "#         if penalising_factor < 1:\n",
    "#             penalising_factor = 1\n",
    "#         return euclidean_distance * penalising_factor\n",
    "\n",
    "# ##################################################################################################################\n",
    "    \n",
    "# # create priors from posterior of SNLE\n",
    "    \n",
    "# posterior_samples_np = posterior_samples.numpy()\n",
    "# parameter_means = np.mean(posterior_samples_np, axis = 0)\n",
    "# parameter_stdevs = np.std(posterior_samples_np, axis = 0)\n",
    "\n",
    "# prior_samples = np.zeros((num_chains, posterior_samples.shape[1]))\n",
    "# for param_idx in range(posterior_samples.shape[1]):\n",
    "#     prior_samples[:, param_idx] = np.random.normal(loc = parameter_means[param_idx], scale = parameter_stdevs[param_idx], size = num_chains)\n",
    "\n",
    "# #####################################################################################################################\n",
    "    \n",
    "# def abc_mcmc_single_chain_seeded(chain, true_params, seed, chain_idx):\n",
    "#     random.seed(seed)\n",
    "#     stored_accepted_data = []\n",
    "#     params = []\n",
    "#     distances = []\n",
    "#     current_simulation = [] # initialised as list but converted to torch.tensor([]) once populated with simulator resuts using current_params, allowing reshaping,\n",
    "#                             # could also be initialised as torch.tensor([])\n",
    "#     starting_params = prior_samples[chain_idx]\n",
    "#     current_params = tuple(starting_params) # required manipulation of the type of object (np.ndarray --> tuple)\n",
    "#     parameter_traces = [current_params]  # Collect parameter traces to plot\n",
    "#     accepted_count = 0\n",
    "#     acceptance_rates = []\n",
    "#     # with tqdm(total=len(chain), desc=f'Chain {chain_idx+1}', position=chain_idx) as pbar:\n",
    "#     for _ in chain:\n",
    "#         proposal_scale = 2.38*(len(true_params) ** -1/2)\n",
    "\n",
    "# # For a Gaussian target distribution of unit variance in each dimension, the optimal σ of the proposal distribution is given as σ_{d}≈2.38d^{−1/2} by Gelman, Roberts,\n",
    "# # and Gilks 1996, where  d is the dimension of the parameter space. For targeting other varieties of Gaussian, we need to scale it appropriately; multiply its\n",
    "# # covariance matrix by the square of σ_{d} given above.\n",
    "        \n",
    "#         while True:\n",
    "#             proposed_params = current_params + np.random.normal(scale=proposal_scale, size=len(true_params))\n",
    "#             if max(proposed_params) > np.max(np.max(posterior_samples_np, axis = 0), axis=0) or min(proposed_params) < np.min(np.min(posterior_samples_np, axis = 0), axis=0):\n",
    "#                 continue  # repeat the current iteration\n",
    "#             break  # exit the while loop if conditions are satisfied\n",
    "#         proposed_simulation = my_simulator_euler(proposed_params)\n",
    "\n",
    "#         proposed_simulation_unflattened = unflatten(proposed_simulation)           \n",
    "#         true_solutions_unflattened = unflatten(true_solutions)\n",
    "\n",
    "#         p_proposed = get_distance(true_solutions_unflattened, proposed_simulation_unflattened)\n",
    "\n",
    "#         if len(current_simulation) == 0:  # if the list is not empty i.e. first iterations of each chain\n",
    "#             current_simulation = my_simulator_euler(current_params)\n",
    "\n",
    "#         current_simulation_unflattened = unflatten(current_simulation)\n",
    "#         p_current = get_distance(true_solutions_unflattened, current_simulation_unflattened)\n",
    "\n",
    "#         if p_proposed < 300:\n",
    "#             acceptance_prob = min(1, -np.exp(-p_current / p_proposed))\n",
    "                                                \n",
    "#             if np.random.rand() < acceptance_prob:\n",
    "#                 accepted_count += 1\n",
    "#                 current_params = proposed_params\n",
    "#                 p_current = p_proposed\n",
    "#                 current_simulation = proposed_simulation\n",
    "#                 stored_accepted_data.append(current_simulation_unflattened)\n",
    "        \n",
    "#         distances.append(p_current)\n",
    "#         params.append(current_params)\n",
    "#         parameter_traces.append(current_params)\n",
    "\n",
    "#         if _ % interval_to_calculate_acceptance_rate == 0:\n",
    "#             acceptance_rates.append(accepted_count / _ * 100)\n",
    "            \n",
    "#         # pbar.update(1)  # Update progress bar\n",
    "    \n",
    "#     return params, stored_accepted_data, parameter_traces, accepted_count, distances, acceptance_rates\n",
    "\n",
    "# def abc_mcmc_multiple_chains(true_params, num_iterations, num_chains, show_progress_bars: bool = True):\n",
    "#     chain_seeds = random.sample(range(0, 1_000_000), num_chains)\n",
    "#     iteration_index = np.arange(1, num_iterations + 1)\n",
    "#     chains = np.array_split(iteration_index, num_chains)\n",
    "#     with tqdm(total=num_iterations, desc=f'Running {num_chains} chains', disable = not show_progress_bars) as pbar: results = Parallel(n_jobs=num_workers)(delayed(abc_mcmc_single_chain_seeded)(chain, true_params, chain_seed, chain_idx) \n",
    "#         for chain_idx, (chain, chain_seed) in enumerate(zip(chains, chain_seeds)))\n",
    "    \n",
    "#     all_params, all_stored_accepted_data, all_parameter_traces, all_accepted_counts, all_distances, all_acceptance_rates = zip(*results)\n",
    "\n",
    "#     return (list(all_params), list(all_stored_accepted_data), list(all_parameter_traces), list(all_accepted_counts), list(all_distances), list(all_acceptance_rates))\n",
    "\n",
    "# all_accepted_parameters, all_accepted_data, all_parameter_traces, all_accepted_counts, all_distances, all_acceptance_rates= abc_mcmc_multiple_chains(true_params, num_iterations, num_chains)\n",
    "\n",
    "# ##################################################################################################################\n",
    "# ##################################################################### TRAJECTORIES ################################\n",
    "\n",
    "# def plot_trajectories(data):\n",
    "#     variables = ['m1', 'p1', 'm2', 'p2', 'm3', 'p3']\n",
    "\n",
    "#     # Initialize lists to store quartiles for each variable\n",
    "#     quartiles = [[] for _ in range(len(variables))]\n",
    "\n",
    "#     # Iterate over each variable\n",
    "#     for variable_idx in range(len(variables)):\n",
    "#         variable_list = []\n",
    "#         # Iterate over each timestep\n",
    "#         for timestep in range(num_timesteps):\n",
    "#             timestep_list = []\n",
    "#             # Iterate over each chain and simulation\n",
    "#             for chain in data:\n",
    "#                 for simulation in chain:\n",
    "#                     timestep_list.append(simulation[timestep][variable_idx])\n",
    "#             variable_list.append(timestep_list)\n",
    "\n",
    "#         # Calculate quartiles for the current variable\n",
    "#         q1_list = []\n",
    "#         q3_list = []\n",
    "#         for timestep_data in variable_list:\n",
    "#             q3, q1 = np.percentile(timestep_data, [75, 25])\n",
    "#             q1_list.append(q1)\n",
    "#             q3_list.append(q3)\n",
    "\n",
    "#         # Append quartile lists for the current variable\n",
    "#         quartiles[variable_idx].extend([q1_list, q3_list])\n",
    "\n",
    "#     # Output quartiles for each variable\n",
    "#     for idx, variable in enumerate(variables):\n",
    "#         q1_list = quartiles[idx][0]\n",
    "#         q3_list = quartiles[idx][1]\n",
    "\n",
    "#     fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "#     for idx, ax in enumerate(axes.flat):\n",
    "#         q1_list = quartiles[idx][0]\n",
    "#         q3_list = quartiles[idx][1]\n",
    "#         true_variable_data = true_solutions_unflattened[:, idx]  # Extract true data for the current variable\n",
    "#         ax.plot(range(num_timesteps), q1_list, label='Q1', color='blue')\n",
    "#         ax.plot(range(num_timesteps), q3_list, label='Q3', color='red')\n",
    "#         ax.fill_between(range(num_timesteps), q1_list, q3_list, alpha=0.3)\n",
    "#         ax.plot(range(num_timesteps), true_variable_data, label='True Data', color='green')\n",
    "#         ax.set_xlabel('t')\n",
    "#         ax.set_ylabel(variables[idx])\n",
    "#         ax.legend()\n",
    "\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "\n",
    "# plot_trajectories(all_accepted_data)\n",
    "\n",
    "# ############################################ PARAMETER TRACES ########################################\n",
    "\n",
    "# fig, axs = plt.subplots(len(true_params), 1, figsize=(15, 3 * len(true_params)), sharex=True)\n",
    "\n",
    "# for i in range(len(true_params)):\n",
    "#     for chain in range(num_chains):\n",
    "#         parameter_trace = np.array(all_parameter_traces[chain])  # Convert to numpy array\n",
    "#         axs[i].plot(parameter_trace[:, i], alpha=1, label=f\"Chain {chain+1}\")\n",
    "#         axs[i].legend()  # Add legend for each subplot\n",
    "    \n",
    "#     axs[i].set_ylabel(headers[i])\n",
    "\n",
    "# axs[-1].set_xlabel('Iteration')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# ####################################################### PAIRPLOTS ###################################\n",
    "\n",
    "# def plot_pairplots(data):\n",
    "#     burn_in_iterations = int(num_iterations / num_chains * burn_in_fraction)\n",
    "#     parameter_accepts = [[] for _ in range(len(headers))]  # List of lists for parameter acceptances\n",
    "#     chain_labels = []\n",
    "\n",
    "#     # Iterate over each chain\n",
    "#     for chain in range(num_chains):\n",
    "#         num_samples = len(data[chain])\n",
    "#         for i in range(burn_in_iterations, num_samples):  # Start from burn-in samples\n",
    "#             accepted_params = data[chain][i]\n",
    "#             for param_idx, param_accept_list in enumerate(parameter_accepts):\n",
    "#                 param_accept_list.append(accepted_params[param_idx])\n",
    "#             chain_labels.append(f\"Chain {chain + 1}\")\n",
    "\n",
    "#     # Construct DataFrame using parameter acceptances and chain labels\n",
    "#     data_dict = {header: accept_list for header, accept_list in zip(headers, parameter_accepts)}\n",
    "#     data_dict[\"Chain\"] = chain_labels\n",
    "#     plot_data = pd.DataFrame(data_dict)\n",
    "\n",
    "#     g = sns.pairplot(plot_data, kind=\"kde\", hue=\"Chain\", palette=\"Set1\")\n",
    "\n",
    "#     # Add vertical/horizontal lines for true parameter values\n",
    "#     for i, j in zip(*np.tril_indices_from(g.axes, -1)):\n",
    "#         for idx, header in enumerate(headers):\n",
    "#             true_value = true_params[idx]\n",
    "#             ax = g.axes[i, j]\n",
    "#             ax.axvline(x=true_value, color='k', linestyle='--')\n",
    "#             if i != j:  # For off-diagonal plots\n",
    "#                 ax.axhline(y=true_value, color='k', linestyle='--')\n",
    "\n",
    "#     for i in range(len(headers)):\n",
    "#         ax = g.axes[i, i]\n",
    "#         for idx, header in enumerate(headers):\n",
    "#             true_value = true_params[idx]\n",
    "#             ax.axvline(x=true_value, color='k', linestyle='--')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# plot_pairplots(all_accepted_parameters)\n",
    "\n",
    "# ############################################################# DISTANCES OVER ITERATION ##############################\n",
    "\n",
    "# fig, axs = plt.subplots(num_chains, 1, figsize=(15, 3 * num_chains), sharex=True)\n",
    "\n",
    "# for chain in range(num_chains):\n",
    "#     axs[chain].plot(all_distances[chain], label=f\"Chain {chain+1}\")\n",
    "#     axs[chain].set_ylabel('Distance')\n",
    "#     axs[chain].set_xlim([0, int(num_iterations/num_chains)])\n",
    "\n",
    "# axs[-1].set_xlabel('Iteration')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# for i in num_chains:\n",
    "#     print(f\"Final distance of Chain {i+1} = {all_distances[i][-1]}\")\n",
    "\n",
    "# ####################################################### ACCEPTANCE RATE #####################################################\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# iterations = range(0, int(num_iterations/num_chains), interval_to_calculate_acceptance_rate)\n",
    "# for chain in range(num_chains):\n",
    "#     plt.plot(iterations, (all_acceptance_rates[chain]))\n",
    "\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Acceptance Rate (%)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
