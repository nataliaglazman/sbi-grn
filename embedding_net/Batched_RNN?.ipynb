{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "\n",
    "# Define repressilator model\n",
    "def model(variables, t, params):\n",
    "\n",
    "    m1, p1, m2, p2, m3, p3 = variables\n",
    "    k1, k2, k3 = params #only ks are parameters to infer\n",
    "    a1 = a2 = a3 = 24.78485282457379\n",
    "    g1 = g2 = g3 = 0.024884149937163258\n",
    "    n1 = n2 = n3 = 5\n",
    "    b1 = b2 = b3 = 33.82307682700831\n",
    "    dm1 = dm2 = dm3 = 1.143402097500176\n",
    "    dp1 = dp2 = dp3 = 0.7833664565550977\n",
    "\n",
    "    dm1dt = -dm1 * m1 + (a1 / (1 + ((1/k1) * p2) ** n1)) + g1\n",
    "    dp1dt = (b1 * m1) - (dp1 * p1)\n",
    "    dm2dt = -dm2 * m2 + (a2 / (1 + ((1/k2) * p3) ** n2)) + g2\n",
    "    dp2dt = (b2 * m2) - (dp2 * p2)\n",
    "    dm3dt = -dm3 * m3 + (a3 / (1 + ((1/k3) * p1) ** n3)) + g3\n",
    "    dp3dt = (b3 * m3)-(dp3 * p3)\n",
    "    \n",
    "    return [dm1dt, dp1dt, dm2dt, dp2dt, dm3dt, dp3dt]\n",
    "\n",
    "# Define the true parameters for k\n",
    "true_params = np.array([\n",
    "    246.96291990024542, 246.96291990024542, 246.96291990024542,])\n",
    "\n",
    "# Establish prior from 0-250\n",
    "num_dim = 3\n",
    "prior = utils.BoxUniform(low=10**-2 * torch.ones(num_dim), high=250 * torch.ones(num_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the time variable (0-100 in 1000 steps), the initial conditions, etc.\n",
    "num_timesteps = 1000\n",
    "num_trajectories = 6\n",
    "initial_conditions = np.array([0, 1, 0, 3, 0, 2])\n",
    "t = np.linspace(0, 100, num_timesteps)\n",
    "\n",
    "\n",
    "t = np.linspace(0, 100, num_timesteps)\n",
    "def my_simulator(theta):\n",
    "    initial_conditions = np.array([0, 2, 0, 1, 0, 3], dtype=np.float32)\n",
    "    solution = odeint(model, initial_conditions, t, args=(theta,))\n",
    "    return torch.tensor(solution, dtype=torch.float32)  # Flatten tensor to size [600]\n",
    "\n",
    "x_o = my_simulator(true_params)\n",
    "x_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size #Number of GRU units\n",
    "        self.input_size = input_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)   \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)  # Number of layers, batch size = 2, number of GRU units\n",
    "        gru_out, _ = self.gru(x, h0)                # GRU layer, we don't store the hn (hidden state output)\n",
    "        output = self.linear(gru_out[:, -1, :])  #(N,L,Dâˆ—H_out when batch_first=True containing the output features (h_t) from the last layer of the RNN, for each t.\n",
    "        return output\n",
    "\n",
    "# Define RNN\n",
    "input_size = 6\n",
    "sequence_length=1000\n",
    "output_size = 25 #We want 1-dimensional embeddings of length = 25 (1x25)\n",
    "hidden_size = 100 #GRU units\n",
    "\n",
    "#reshaped data should be [batch, sequence_length, input_size] - [batch, 1000,6]\n",
    "embedding_net = RNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input tensor shape [batch_size, sequence_length, input_size]\n",
    "dummy_input = torch.randn(2, sequence_length, input_size)  # Assuming batch size of 2\n",
    "# Pass the input through the embedding network\n",
    "output = embedding_net(dummy_input)\n",
    "# Print the output size\n",
    "print(\"Output size:\", output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prepare for sbi function checks simulator and prior are compatible with the NPE algorithm (and makes sure it adds a batch dimension)\n",
    "simulator_wrapper, prior = prepare_for_sbi(my_simulator, prior)\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=10) # Simulate data\n",
    "print(theta.shape,x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We define the neural network (neural density estimator), specifying the embedding net. In this case we use a mixture density network.\n",
    "neural_posterior = utils.posterior_nn(\n",
    "    model=\"mdn\", embedding_net=embedding_net, hidden_features=10, num_transforms=2)\n",
    "\n",
    "# Setup the inference procedure with the SNPE-C (Greenberg et al, 2019)\n",
    "inference = SNPE(prior=prior, density_estimator=neural_posterior)\n",
    "\n",
    "density_estimator = inference.append_simulations(theta, x).train(training_batch_size=2)        # Train density estimator\n",
    "posterior = inference.build_posterior(density_estimator)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then sample the posterior and plot\n",
    "posterior_samples = posterior.sample((100,), x=x_o)\n",
    "_ = analysis.pairplot(\n",
    "    posterior_samples, limits=[[-100, 300], [-100, 300], [-100, 300]], figsize=(6, 6) \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
