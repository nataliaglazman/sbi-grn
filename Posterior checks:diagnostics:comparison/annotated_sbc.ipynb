{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostics and Comparison (C2ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/E_V_/Documents/Uni/PostGrad/Project_1/sbi_env1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import eye, ones, zeros\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "import random\n",
    "import time\n",
    "\n",
    "import sbi\n",
    "\n",
    "from sbi.analysis import check_sbc, run_sbc, get_nltp, sbc_rank_plot\n",
    "from sbi.inference import SNRE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi.simulators import linear_gaussian, diagonal_linear_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_params = 0., 0.       # if this changes, model must also be adjusted\n",
    "prior_min = -3\n",
    "prior_max = 3\n",
    "num_timesteps = 100\n",
    "num_simulations = 1000   # how many simulations in each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repressilator(variables, t, theta):\n",
    "    m1, p1, m2, p2, m3, p3 = variables\n",
    "    k1, k2 = theta\n",
    "    return [-m1 + (10 ** 3 / (1 + (10 ** k1 * p2) ** 2)) + 10 ** 0, #return the results if the six odes\n",
    "            -10 ** 0 * (p1 - m1),\n",
    "            -m2 + (10 ** 3 / (1 + (10 ** k2 * p3) ** 2)) + 10 ** 0,\n",
    "            -10 ** 0 * (p2 - m2),\n",
    "            -m3 + (10 ** 3 / (1 + (10 ** 0 * p1) ** 2)) + 10 ** 0,\n",
    "            -10 ** 0 * (p3 - m3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 100, num_timesteps)\n",
    "def my_simulator(theta):\n",
    "    initial_conditions = np.array([0, 2, 0, 1, 0, 3], dtype=np.float32)\n",
    "    solution = odeint(repressilator, initial_conditions, t, args=(theta,))\n",
    "    return torch.tensor(solution, dtype=torch.float32).flatten()  # Flat\n",
    "num_dim = 2\n",
    "prior = utils.BoxUniform(low=prior_min * torch.ones(num_dim), high=prior_max * torch.ones(num_dim))\n",
    "inference = SNRE(prior=prior)\n",
    "simulator, prior = prepare_for_sbi(my_simulator, prior) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 1000 simulations.: 100%|██████████| 1000/1000 [02:20<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 69 epochs."
     ]
    }
   ],
   "source": [
    "posteriors = []\n",
    "theta, x = simulate_for_sbi(simulator, prior, num_simulations=1000)\n",
    "# In `SNLE` and `SNRE`, you should not pass the `proposal` to `.append_simulations()`\n",
    "density_estimator = inference.append_simulations(\n",
    "    theta, x\n",
    ").train()\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "posteriors.append(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [[2.5600452 1.6042233]]\n",
      "x    : [[  0.           2.           0.           1.           0.\n",
      "    3.           0.6457766    0.9999446    0.6401354    0.63595146\n",
      "  248.25851     90.248764     0.88136154   0.8743084    0.86895746\n",
      "    0.73504823 442.9172     269.87436      0.9642121    0.91388994\n",
      "    0.9522798    0.85530543 515.2523     411.59006      0.99289846\n",
      "    0.958053     0.98262304   0.9297513  525.52747    483.01422\n",
      "    1.0027109    0.98437697   0.9936732    0.968025   518.0667\n",
      "  507.57108      1.0060122    0.9974396    0.9976974    0.98602855\n",
      "  509.51926    510.9662       1.0070974    1.0033537    0.999163\n",
      "    0.99406546 503.69705    507.73724      1.0074425    1.005882\n",
      "    0.99969673   0.9975315  500.3956     503.8385       1.0075473\n",
      "    1.0069194    0.9998911    0.9989901  498.70068    500.96024\n",
      "    1.0075766    1.0073314    0.9999619    0.9995928  497.88498\n",
      "  499.18976      1.0075837    1.0074905    0.9999877    0.99983835\n",
      "  497.51028    498.2061       1.0075849    1.0075504    0.99999714\n",
      "    0.9999373  497.34424    497.69513      1.0075848    1.0075723\n",
      "    1.0000006    0.99997675 497.27277    497.44244      1.0075845\n",
      "    1.0075802    1.0000018    0.99999243 497.24277    497.3221\n",
      "    1.0075843    1.0075828    1.0000023    0.99999857 497.23047\n",
      "  497.26654      1.0075842    1.0075837    1.0000024    1.000001\n",
      "  497.2255     497.24155      1.0075841    1.007584     1.0000025\n",
      "    1.0000019  497.22354    497.23053      1.0075841    1.0075841\n",
      "    1.0000025    1.0000023  497.22278    497.22577      1.0075841\n",
      "    1.0075841    1.0000025    1.0000024  497.2225     497.22375\n",
      "    1.0075841    1.0075841    1.0000025    1.0000024  497.22238\n",
      "  497.2229       1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22235    497.22256      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.2224       1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22235      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232\n",
      "    1.0075841    1.0075841    1.0000025    1.0000025  497.22232\n",
      "  497.22232      1.0075841    1.0075841    1.0000025    1.0000025\n",
      "  497.22232    497.22232      1.0075841    1.0075841    1.0000025\n",
      "    1.0000025  497.22232    497.22232      1.0075841    1.0075841\n",
      "    1.0000025    1.0000025  497.22232    497.22232      1.0075841\n",
      "    1.0075841    1.0000025    1.0000025  497.22232    497.22232   ]]\n"
     ]
    }
   ],
   "source": [
    "theta_o = prior.sample((1,))\n",
    "x_o = simulator(theta_o)\n",
    "print(\"theta:\", theta_o.numpy())\n",
    "print(\"x    :\", x_o.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning bracket width...: 100%|██████████| 50/50 [00:00<00:00, 288.66it/s]\n",
      "Generating samples: 100%|██████████| 10100/10100 [03:17<00:00, 51.06it/s]\n"
     ]
    }
   ],
   "source": [
    "posterior_samples = posterior.sample((1_000,), x=x_o)\n",
    "# Generate predictive samples by simulating from posterior samples.\n",
    "posterior_predictive_samples = simulator(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some posterior predictive checks to see if the\n",
    "# posterior predictive samples cluster aournd the observation `x_o`.\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    samples=posterior_predictive_samples,\n",
    "    points=x_o,\n",
    "    limits=list(zip(x_o.flatten() - 1.0, x_o.flatten() + 1.0)),\n",
    "    offdiag=\"kde\",\n",
    "    diag=\"kde\",\n",
    "    figsize=(5, 5),\n",
    "    labels=[rf\"$x_{d}$\" for d in range(3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sbc_runs = 100  # choose a number of sbc runs, should be ~100s or ideally 1000\n",
    "# generate ground truth parameters and corresponding simulated observations for SBC.\n",
    "thetas = prior.sample((num_sbc_runs,))\n",
    "xs = simulator(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/E_V_/Documents/Uni/PostGrad/Project_1/sbi_env1/lib/python3.11/site-packages/sbi/analysis/sbc.py:57: UserWarning: Number of SBC samples should be on the order of 100s to give realiable\n",
      "            results. We recommend using 300.\n",
      "  warnings.warn(\n",
      "Running 100 sbc samples.: 100%|██████████| 100/100 [30:31<00:00, 18.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# run SBC: for each inference we draw 1000 posterior samples.\n",
    "num_posterior_samples = 100\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas, xs, posterior, num_posterior_samples=num_posterior_samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/E_V_/Documents/Uni/PostGrad/Project_1/sbi_env1/lib/python3.11/site-packages/sbi/analysis/sbc.py:363: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
      "  if (c2st_scores.std(0) > 0.05).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kolmogorov-smirnov p-values \n",
      "check_stats['ks_pvals'] = [0.00263005 0.16496272]\n"
     ]
    }
   ],
   "source": [
    "check_stats = check_sbc(\n",
    "    ranks, thetas, dap_samples, num_posterior_samples=num_posterior_samples\n",
    ")\n",
    "print(\n",
    "    f\"kolmogorov-smirnov p-values \\ncheck_stats['ks_pvals'] = {check_stats['ks_pvals'].numpy()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p values are for the null hypothesis that the samples from ranks are drawn from a uniform distribution (in other words H_0: PDF(ranks) == PDF(uniform)).\n",
    "We have two parameters - one p-value for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sbi.analysis import sbc_rank_plot\n",
    "\n",
    "f, ax = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=50,  # by passing None we use a heuristic for the number of bins.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighted in grey you see the 99% confidence interval of a uniform distribution given the number of samples provided. I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a long time - we will need either hpc or doing it on amortised model (no sequential stuff), that would take quite long!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two results visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. We have posterior_samples from SNLE and NRE in np.array posterior_samples and posterior_samples sre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    " \n",
    "\n",
    "column_name = ['k1','k2']\n",
    "\n",
    "snle_pos = pd.DataFrame(data=posterior_samples,columns=column_name)\n",
    "snre_pos = pd.DataFrame(data=posterior_samples_sre, columns=column_name)\n",
    "\n",
    "snle_pos['algorithm']=\"SNLE\"\n",
    "snre_pos[\"algorithm\"]=\"SNRE\"\n",
    "df_joint = pd.concat([snle_pos, snre_pos])\n",
    "g=sns.PairGrid(df_joint, diag_sharey=False, hue=\"algorithm\")\n",
    "g.map_diag(sns.kdeplot, fill=True, alpha=0.2)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_upper(sns.scatterplot,s=3)\n",
    "g.add_legend()\n",
    "sns.move_legend(\n",
    "    g, \"lower center\",\n",
    "    bbox_to_anchor=(.5, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2ST\n",
    "\n",
    "\n",
    "A classifier-based two sample test\n",
    "A MLP model is used with 2 hidden layers, hyperparameters including learning rate and the number of nodes in each hidden layer.\n",
    "Accuracy of classifier on test data is returned. Accuracy is considered a good metric for classifier performance as the data is balanced across the two groups.\n",
    "\n",
    "at or below 0.5 meaning indistinguishable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c2st(\n",
    "    X: torch.Tensor,\n",
    "    Y: torch.Tensor,\n",
    "    seed: int = 1,\n",
    "    n_folds: int = 5\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Classifier-based 2-sample test returning accuracy, modified from SBIBM\n",
    "\n",
    "    Args:\n",
    "        X: Sample 1\n",
    "        Y: Sample 2\n",
    "        seed: Seed for sklearn\n",
    "        n_folds: Number of folds. Initially set to 5\n",
    "       \n",
    "    \"\"\"\n",
    "    ndim = X.shape[1]\n",
    "    data = np.concatenate((X, Y))\n",
    "    target = np.concatenate(\n",
    "        (\n",
    "            np.zeros((X.shape[0],)),\n",
    "            np.ones((Y.shape[0],)),\n",
    "        )\n",
    "    )\n",
    "    X_train, X_test, y_train, y_test=sklearn.model_selection.train_test_split(data, target, test_size=0.2, random_state=None, shuffle=True)\n",
    "    mlp = MLPClassifier(max_iter=10000)\n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [(ndim,ndim), (2*ndim,2*ndim)],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "    shuffle = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=shuffle)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #means = clf.cv_results_['mean_test_score']\n",
    "    #stds = clf.cv_results_['std_test_score']\n",
    "    #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    #print('Best parameters found:\\n', clf.best_params_)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    score=clf.score(X_test, y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive check (PPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trajectories quantile\n",
    "raw_trajectories=np.zeros([100,100,6]) #first dim:no of samples, 100 time points, 6 trajec\n",
    "for i in range(posterior_samples.shape[0]):\n",
    "    raw_trajectories[i]=my_simulator(posterior_samples[i,:]).reshape(100,6)\n",
    "tr=np.percentile(raw_trajectories, [2.5, 97.5], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax=plt.subplots(2,3,figsize=(12,9))\n",
    "ax = ax.ravel()\n",
    "col=[\"blue\",\"blue\"]\n",
    "titles=[\"m1\",\"p1\",\"m2\",\"p2\", \"m3\", \"p3\"]\n",
    "for i in range(6):\n",
    "    for j in range(2):\n",
    "        ax[i].plot(tr[j,:,i],c=col[j],alpha=0.61)\n",
    "    ax[i].plot(x_o.reshape(100,6)[:,i],c=\"purple\")\n",
    "    ax[i].fill_between(tr[0, :, i],tr[1, :, i],alpha=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
